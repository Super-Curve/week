# -*- coding: utf-8 -*-
"""
ä¼ä¸šçº§é«˜ä½ç‚¹åˆ†æå™¨ - èåˆé¡¶çº§é‡åŒ–äº¤æ˜“æŠ€æœ¯çš„æ™ºèƒ½è½¬æŠ˜ç‚¹è¯†åˆ«ç³»ç»Ÿ
é›†æˆåˆ†å½¢ç»´åº¦ã€å¤šæ—¶é—´æ¡†æ¶ã€ç»Ÿè®¡éªŒè¯ã€æœºå™¨å­¦ä¹ ã€å¸‚åœºå¾®è§‚ç»“æ„ç­‰å…ˆè¿›æŠ€æœ¯
"""

import numpy as np
import pandas as pd
from scipy import stats
from scipy.signal import argrelextrema, find_peaks
from scipy.optimize import minimize_scalar
import warnings
warnings.filterwarnings('ignore')

try:
    import talib
    TALIB_AVAILABLE = True
except ImportError:
    TALIB_AVAILABLE = False
    print("Warning: TA-Lib not available. Using numpy implementations.")

try:
    from sklearn.ensemble import IsolationForest, RandomForestClassifier
    from sklearn.preprocessing import StandardScaler, RobustScaler
    from sklearn.cluster import DBSCAN
    from sklearn.metrics import silhouette_score
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False
    print("Warning: Scikit-learn not available. ML features disabled.")


class EnterprisesPivotAnalyzer:
    """
    ä¼ä¸šçº§é«˜ä½ç‚¹åˆ†æå™¨ - èåˆå¤šç§é¡¶çº§é‡åŒ–äº¤æ˜“æŠ€æœ¯
    
    æ ¸å¿ƒæŠ€æœ¯æ ˆï¼š
    1. åˆ†å½¢ç»´åº¦åˆ†æ - åŸºäºåˆ†å½¢å‡ ä½•çš„è½¬æŠ˜ç‚¹è¯†åˆ«
    2. å¤šæ—¶é—´æ¡†æ¶ç¡®è®¤ - ç¡®ä¿ä¸åŒæ—¶é—´å°ºåº¦çš„ä¸€è‡´æ€§
    3. ç»Ÿè®¡æ˜¾è‘—æ€§éªŒè¯ - ä½¿ç”¨ç»Ÿè®¡å­¦æ–¹æ³•éªŒè¯è½¬æŠ˜ç‚¹
    4. åŠ¨æ€é˜ˆå€¼è°ƒæ•´ - åŸºäºå¸‚åœºçŠ¶æ€çš„è‡ªé€‚åº”é˜ˆå€¼
    5. æœºå™¨å­¦ä¹ å¢å¼º - ä½¿ç”¨é›†æˆå­¦ä¹ æ–¹æ³•
    6. å¸‚åœºå¾®è§‚ç»“æ„ - åŸºäºæˆäº¤é‡å’Œä»·æ ¼è¡Œä¸ºåˆ†æ
    7. æ³¢åŠ¨ç‡åˆ¶åº¦æ£€æµ‹ - è¯†åˆ«ä¸åŒçš„æ³¢åŠ¨ç‡ç¯å¢ƒ
    """
    
    def __init__(self):
        self.talib_available = TALIB_AVAILABLE
        self.ml_available = ML_AVAILABLE
        
        # é…ç½®å‚æ•°
        self.config = {
            'fractal_dimension_window': 20,
            'statistical_significance_level': 0.05,
            'volatility_regime_lookback': 50,
            'multi_timeframe_weights': [0.4, 0.3, 0.2, 0.1],  # æƒé‡åˆ†é…
            'ml_ensemble_size': 5,
            'microstructure_window': 10
        }
        
    def detect_pivot_points(self, data, method='enterprise_ensemble', sensitivity='balanced', **kwargs):
        """
        ç»Ÿä¸€çš„é«˜ä½ç‚¹æ£€æµ‹æ¥å£ - ä¼ä¸šçº§æ£€æµ‹ç³»ç»Ÿ
        
        Args:
            data: pandas DataFrameï¼ŒåŒ…å«OHLCVæ•°æ®
            method: str, æ£€æµ‹æ–¹æ³•
                - 'enterprise_ensemble': ä¼ä¸šçº§é›†æˆæ–¹æ³•ï¼ˆæ¨èï¼‰
                - 'fractal_dimension': åˆ†å½¢ç»´åº¦åˆ†æ
                - 'multi_timeframe': å¤šæ—¶é—´æ¡†æ¶ç¡®è®¤
                - 'statistical_significance': ç»Ÿè®¡æ˜¾è‘—æ€§éªŒè¯
                - 'adaptive_ml': è‡ªé€‚åº”æœºå™¨å­¦ä¹ 
                - 'microstructure': å¸‚åœºå¾®è§‚ç»“æ„åˆ†æ
            sensitivity: str, æ•æ„Ÿåº¦ ['conservative', 'balanced', 'aggressive']
            
        Returns:
            dict: æ ‡å‡†åŒ–çš„åˆ†æç»“æœï¼Œå…¼å®¹åŸæœ‰æ¥å£
        """
        if len(data) < 30:
            return self._create_empty_result("æ•°æ®é•¿åº¦ä¸è¶³")
            
        try:
            print(f"ğŸš€ å¯åŠ¨ä¼ä¸šçº§é«˜ä½ç‚¹æ£€æµ‹ç³»ç»Ÿ - æ–¹æ³•: {method}")
            
            # 1. æ•°æ®é¢„å¤„ç†å’Œè´¨é‡æ£€æŸ¥
            processed_data = self._preprocess_data(data)
            if processed_data is None:
                return self._create_empty_result("æ•°æ®é¢„å¤„ç†å¤±è´¥")
            
            # 2. è®¡ç®—å…¨æ–¹ä½æŠ€æœ¯æŒ‡æ ‡å¥—ä»¶
            technical_suite = self._calculate_technical_suite(processed_data)
            
            # 3. æ ¹æ®æ–¹æ³•é€‰æ‹©æ£€æµ‹ç­–ç•¥
            if method == 'enterprise_ensemble':
                pivot_results = self._enterprise_ensemble_detection(processed_data, technical_suite, sensitivity)
            elif method == 'fractal_dimension':
                pivot_results = self._fractal_dimension_detection(processed_data, technical_suite, sensitivity)
            elif method == 'multi_timeframe':
                pivot_results = self._multi_timeframe_detection(processed_data, technical_suite, sensitivity)
            elif method == 'statistical_significance':
                pivot_results = self._statistical_significance_detection(processed_data, technical_suite, sensitivity)
            elif method == 'adaptive_ml':
                pivot_results = self._adaptive_ml_detection(processed_data, technical_suite, sensitivity)
            elif method == 'microstructure':
                pivot_results = self._microstructure_detection(processed_data, technical_suite, sensitivity)
            else:
                # å‘åå…¼å®¹æ—§æ–¹æ³•
                pivot_results = self._legacy_detection(processed_data, technical_suite, method, sensitivity)
            
            # 4. è´¨é‡è¯„ä¼°å’ŒéªŒè¯
            quality_metrics = self._comprehensive_quality_assessment(pivot_results, processed_data, technical_suite)
            
            # 5. ç”Ÿæˆä¼ä¸šçº§æŠ¥å‘Š
            analysis_report = self._generate_enterprise_report(
                pivot_results, technical_suite, quality_metrics, method, sensitivity
            )
            
            # 6. æ ‡å‡†åŒ–è¾“å‡ºæ ¼å¼ï¼ˆå…¼å®¹åŸæœ‰æ¥å£ï¼‰
            return self._standardize_output(
                pivot_results, technical_suite, quality_metrics, analysis_report, method
            )
            
        except Exception as e:
            print(f"âŒ ä¼ä¸šçº§é«˜ä½ç‚¹æ£€æµ‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return self._create_empty_result(f"æ£€æµ‹å¤±è´¥: {str(e)}")
    
    # ä¸ºäº†å…¼å®¹æ€§ï¼Œä¿ç•™æ—§æ–¹æ³•å
    def detect_advanced_pivots(self, data, method='enterprise_ensemble', sensitivity='balanced'):
        """å…¼å®¹æ€§æ–¹æ³•ï¼Œè°ƒç”¨æ–°çš„ç»Ÿä¸€æ¥å£"""
        return self.detect_pivot_points(data, method, sensitivity)
    
    # ========================= æ ¸å¿ƒæ£€æµ‹æ–¹æ³• =========================
    
    def _preprocess_data(self, data):
        """ä¼ä¸šçº§æ•°æ®é¢„å¤„ç†å’Œè´¨é‡æ£€æŸ¥"""
        try:
            # ç¡®ä¿æ•°æ®å®Œæ•´æ€§
            required_cols = ['open', 'high', 'low', 'close']
            for col in required_cols:
                if col not in data.columns:
                    print(f"âŒ ç¼ºå°‘å¿…è¦åˆ—: {col}")
                    return None
            
            # æ•°æ®æ¸…æ´—
            processed = data.copy()
            
            # å¤„ç†å¼‚å¸¸å€¼ï¼ˆä½¿ç”¨IQRæ–¹æ³•ï¼‰
            for col in required_cols:
                Q1 = processed[col].quantile(0.25)
                Q3 = processed[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                processed[col] = processed[col].clip(lower_bound, upper_bound)
            
            # ç¡®ä¿OHLCé€»è¾‘æ­£ç¡®æ€§
            processed['high'] = processed[['open', 'high', 'low', 'close']].max(axis=1)
            processed['low'] = processed[['open', 'high', 'low', 'close']].min(axis=1)
            
            # å¡«å……ç¼ºå¤±å€¼
            processed = processed.fillna(method='ffill').fillna(method='bfill')
            
            return processed
            
        except Exception as e:
            print(f"âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}")
            return None
    
    def _calculate_technical_suite(self, data):
        """è®¡ç®—å…¨æ–¹ä½æŠ€æœ¯æŒ‡æ ‡å¥—ä»¶"""
        suite = {}
        
        high_prices = data['high'].values
        low_prices = data['low'].values
        close_prices = data['close'].values
        volume = data.get('volume', pd.Series(index=data.index, dtype=float)).values
        
        # 1. æ³¢åŠ¨ç‡æŒ‡æ ‡æ—
        suite['volatility'] = self._calculate_volatility_suite(high_prices, low_prices, close_prices)
        
        # 2. è¶‹åŠ¿æŒ‡æ ‡æ—
        suite['trend'] = self._calculate_trend_suite(high_prices, low_prices, close_prices)
        
        # 3. åŠ¨é‡æŒ‡æ ‡æ—
        suite['momentum'] = self._calculate_momentum_suite(high_prices, low_prices, close_prices)
        
        # 4. æˆäº¤é‡æŒ‡æ ‡æ—
        suite['volume'] = self._calculate_volume_suite(close_prices, volume)
        
        # 5. å¸‚åœºç»“æ„æŒ‡æ ‡
        suite['structure'] = self._calculate_structure_suite(high_prices, low_prices, close_prices)
        
        # 6. åˆ†å½¢å’Œç»Ÿè®¡æŒ‡æ ‡
        suite['fractal'] = self._calculate_fractal_suite(close_prices)
        
        return suite
    
    def _calculate_volatility_suite(self, high_prices, low_prices, close_prices):
        """è®¡ç®—é«˜çº§æ³¢åŠ¨ç‡æŒ‡æ ‡å¥—ä»¶"""
        volatility_suite = {}
        
        # 1. ATR å®¶æ—
        for period in [7, 14, 21]:
            if self.talib_available:
                try:
                    # ç¡®ä¿æ•°æ®ç±»å‹ä¸ºfloat64
                    high_float = np.array(high_prices, dtype=np.float64)
                    low_float = np.array(low_prices, dtype=np.float64)
                    close_float = np.array(close_prices, dtype=np.float64)
                    atr = talib.ATR(high_float, low_float, close_float, timeperiod=period)
                except Exception:
                    # å¦‚æœTA-Libå¤±è´¥ï¼Œä½¿ç”¨æ‰‹åŠ¨è®¡ç®—
                    tr = self._calculate_true_range(high_prices, low_prices, close_prices)
                    atr = pd.Series(tr).rolling(window=period).mean().values
            else:
                tr = self._calculate_true_range(high_prices, low_prices, close_prices)
                atr = pd.Series(tr).rolling(window=period).mean().values
            volatility_suite[f'atr_{period}'] = atr
            volatility_suite[f'atr_{period}_pct'] = (atr / close_prices) * 100
        
        # 2. é«˜çº§æ³¢åŠ¨ç‡ä¼°è®¡å™¨
        try:
            # Garman-Klass ä¼°è®¡å™¨
            gk_vol = self._calculate_garman_klass_volatility(high_prices, low_prices, close_prices)
            volatility_suite['garman_klass'] = gk_vol
            
            # Parkinson ä¼°è®¡å™¨
            parkinson_vol = np.sqrt(252 / (4 * np.log(2))) * np.sqrt(np.log(high_prices / low_prices) ** 2)
            volatility_suite['parkinson'] = parkinson_vol
            
        except Exception as e:
            print(f"é«˜çº§æ³¢åŠ¨ç‡è®¡ç®—è­¦å‘Š: {e}")
            # ä½¿ç”¨åŸºç¡€ATRä½œä¸ºåå¤‡
            volatility_suite['garman_klass'] = volatility_suite.get('atr_14_pct', np.ones(len(close_prices)) * 5)
            volatility_suite['parkinson'] = volatility_suite.get('atr_14_pct', np.ones(len(close_prices)) * 5)
        
        # 3. åŠ¨æ€é˜ˆå€¼
        volatility_suite['dynamic_threshold'] = np.nanpercentile(volatility_suite['atr_14_pct'], 75)
        
        # 4. æ³¢åŠ¨ç‡åˆ¶åº¦åˆ†ç±»
        volatility_suite['regime'] = self._classify_volatility_regime(volatility_suite['atr_14_pct'])
        
        return volatility_suite
    
    def _calculate_trend_suite(self, high_prices, low_prices, close_prices):
        """è®¡ç®—è¶‹åŠ¿æŒ‡æ ‡æ—"""
        trend_suite = {}
        
        # 1. ç§»åŠ¨å¹³å‡çº¿æ—
        for period in [5, 10, 20, 50]:
            if self.talib_available:
                try:
                    close_float = np.array(close_prices, dtype=np.float64)
                    ma = talib.SMA(close_float, timeperiod=period)
                except Exception:
                    ma = pd.Series(close_prices).rolling(window=period).mean().values
            else:
                ma = pd.Series(close_prices).rolling(window=period).mean().values
            trend_suite[f'ma_{period}'] = ma
        
        # 2. è¶‹åŠ¿å¼ºåº¦
        if self.talib_available:
            try:
                high_float = np.array(high_prices, dtype=np.float64)
                low_float = np.array(low_prices, dtype=np.float64)
                close_float = np.array(close_prices, dtype=np.float64)
                trend_suite['adx'] = talib.ADX(high_float, low_float, close_float, timeperiod=14)
            except Exception:
                # ADXè®¡ç®—å¤±è´¥ï¼Œè·³è¿‡
                pass
        
        # 3. ä»·æ ¼ç›¸å¯¹ä½ç½®
        trend_suite['price_position'] = (close_prices - trend_suite['ma_20']) / trend_suite['ma_20']
        
        return trend_suite
    
    def _calculate_momentum_suite(self, high_prices, low_prices, close_prices):
        """è®¡ç®—åŠ¨é‡æŒ‡æ ‡æ—"""
        momentum_suite = {}
        
        # 1. RSI
        if self.talib_available:
            try:
                close_float = np.array(close_prices, dtype=np.float64)
                momentum_suite['rsi'] = talib.RSI(close_float, timeperiod=14)
            except Exception:
                # RSIè®¡ç®—å¤±è´¥ï¼Œè·³è¿‡
                pass
        
        # 2. ä»·æ ¼åŠ¨é‡
        momentum_suite['momentum_5'] = (close_prices - np.roll(close_prices, 5)) / np.roll(close_prices, 5)
        momentum_suite['momentum_10'] = (close_prices - np.roll(close_prices, 10)) / np.roll(close_prices, 10)
        
        return momentum_suite
    
    def _calculate_volume_suite(self, close_prices, volume):
        """è®¡ç®—æˆäº¤é‡æŒ‡æ ‡æ—"""
        volume_suite = {}
        
        if np.any(volume > 0):
            volume_suite['volume_available'] = True
            volume_suite['volume_ma'] = pd.Series(volume).rolling(window=20).mean().values
            volume_suite['relative_volume'] = volume / volume_suite['volume_ma']
            
            if self.talib_available:
                try:
                    # ç¡®ä¿æ•°æ®ç±»å‹ä¸ºfloat64ï¼ŒTA-Libè¦æ±‚
                    close_prices_float = np.array(close_prices, dtype=np.float64)
                    volume_float = np.array(volume, dtype=np.float64)
                    volume_suite['obv'] = talib.OBV(close_prices_float, volume_float)
                except Exception as e:
                    print(f"âš ï¸  OBVè®¡ç®—å‡ºé”™ï¼Œè·³è¿‡: {e}")
                    # ä¸è®¾ç½®OBVï¼Œç»§ç»­å…¶ä»–è®¡ç®—
        else:
            volume_suite['volume_available'] = False
        
        return volume_suite
    
    def _calculate_structure_suite(self, high_prices, low_prices, close_prices):
        """è®¡ç®—å¸‚åœºç»“æ„æŒ‡æ ‡"""
        structure_suite = {}
        
        # 1. æ”¯æ’‘é˜»åŠ›å¼ºåº¦
        structure_suite['support_strength'] = self._calculate_support_resistance_strength(low_prices, 'support')
        structure_suite['resistance_strength'] = self._calculate_support_resistance_strength(high_prices, 'resistance')
        
        # 2. ä»·æ ¼å¯†åº¦
        structure_suite['price_density'] = self._calculate_price_density(close_prices)
        
        return structure_suite
    
    def _calculate_fractal_suite(self, close_prices):
        """è®¡ç®—åˆ†å½¢å’Œç»Ÿè®¡æŒ‡æ ‡"""
        fractal_suite = {}
        
        # 1. åˆ†å½¢ç»´åº¦ï¼ˆHurstæŒ‡æ•°ï¼‰
        fractal_suite['hurst_exponent'] = self._calculate_hurst_exponent(close_prices)
        
        # 2. åˆ†å½¢ç»´åº¦
        fractal_suite['fractal_dimension'] = 2 - fractal_suite['hurst_exponent']
        
        return fractal_suite
    
    # ========================= æ£€æµ‹ç­–ç•¥å®ç° =========================
    
    def _enterprise_ensemble_detection(self, data, technical_suite, sensitivity):
        """ä¼ä¸šçº§é›†æˆæ£€æµ‹æ–¹æ³•"""
        print("ğŸ“Š æ‰§è¡Œä¼ä¸šçº§é›†æˆæ£€æµ‹...")
        
        high_prices = data['high'].values
        low_prices = data['low'].values
        
        # è·å–æ•æ„Ÿåº¦å‚æ•°
        params = self._get_sensitivity_params(sensitivity)
        
        # 1. åŸºç¡€æå€¼æ£€æµ‹
        raw_highs, raw_lows = self._find_raw_pivot_points(high_prices, low_prices, params['min_distance'])
        
        # 2. å¤šç»´åº¦è¯„åˆ†
        high_candidates = []
        low_candidates = []
        
        for idx in raw_highs:
            score = self._calculate_enterprise_score(idx, high_prices, True, technical_suite, params)
            if score >= params['score_threshold']:
                high_candidates.append((idx, score))
        
        for idx in raw_lows:
            score = self._calculate_enterprise_score(idx, low_prices, False, technical_suite, params)
            if score >= params['score_threshold']:
                low_candidates.append((idx, score))
        
        # 3. æ’åºå¹¶é€‰æ‹©æœ€ä½³å€™é€‰
        high_candidates.sort(key=lambda x: x[1], reverse=True)
        low_candidates.sort(key=lambda x: x[1], reverse=True)
        
        return {
            'raw_pivot_highs': raw_highs,
            'raw_pivot_lows': raw_lows,
            'filtered_pivot_highs': [x[0] for x in high_candidates],
            'filtered_pivot_lows': [x[0] for x in low_candidates],
            'pivot_scores': {'highs': high_candidates, 'lows': low_candidates}
        }
    
    def _fractal_dimension_detection(self, data, technical_suite, sensitivity):
        """åŸºäºåˆ†å½¢ç»´åº¦çš„æ£€æµ‹"""
        print("ğŸ” æ‰§è¡Œåˆ†å½¢ç»´åº¦åˆ†æ...")
        
        high_prices = data['high'].values
        low_prices = data['low'].values
        params = self._get_sensitivity_params(sensitivity)
        
        # åŸºç¡€æ£€æµ‹
        raw_highs, raw_lows = self._find_raw_pivot_points(high_prices, low_prices, params['min_distance'])
        
        # åˆ†å½¢ç»´åº¦è¿‡æ»¤
        fractal_info = technical_suite['fractal']
        hurst_threshold = 0.5  # Hurst > 0.5 è¡¨ç¤ºè¶‹åŠ¿æ€§ï¼Œ< 0.5 è¡¨ç¤ºå‡å€¼å›å½’
        
        filtered_highs = []
        filtered_lows = []
        
        for idx in raw_highs:
            # æ£€æŸ¥å±€éƒ¨åˆ†å½¢ç‰¹å¾
            local_hurst = self._calculate_local_hurst(high_prices, idx, window=10)
            if local_hurst < hurst_threshold:  # å‡å€¼å›å½’ç¯å¢ƒä¸­çš„é«˜ç‚¹æ›´å¯é 
                filtered_highs.append(idx)
        
        for idx in raw_lows:
            local_hurst = self._calculate_local_hurst(low_prices, idx, window=10)
            if local_hurst < hurst_threshold:  # å‡å€¼å›å½’ç¯å¢ƒä¸­çš„ä½ç‚¹æ›´å¯é 
                filtered_lows.append(idx)
        
        return {
            'raw_pivot_highs': raw_highs,
            'raw_pivot_lows': raw_lows,
            'filtered_pivot_highs': filtered_highs,
            'filtered_pivot_lows': filtered_lows
        }
    
    def _statistical_significance_detection(self, data, technical_suite, sensitivity):
        """åŸºäºç»Ÿè®¡æ˜¾è‘—æ€§çš„æ£€æµ‹"""
        print("ğŸ“ˆ æ‰§è¡Œç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ...")
        
        high_prices = data['high'].values
        low_prices = data['low'].values
        params = self._get_sensitivity_params(sensitivity)
        
        raw_highs, raw_lows = self._find_raw_pivot_points(high_prices, low_prices, params['min_distance'])
        
        filtered_highs = []
        filtered_lows = []
        
        # å¯¹æ¯ä¸ªå€™é€‰ç‚¹è¿›è¡Œç»Ÿè®¡æ£€éªŒ
        for idx in raw_highs:
            if self._is_statistically_significant(high_prices, idx, True):
                filtered_highs.append(idx)
        
        for idx in raw_lows:
            if self._is_statistically_significant(low_prices, idx, False):
                filtered_lows.append(idx)
        
        return {
            'raw_pivot_highs': raw_highs,
            'raw_pivot_lows': raw_lows,
            'filtered_pivot_highs': filtered_highs,
            'filtered_pivot_lows': filtered_lows
        }
    
    def _adaptive_ml_detection(self, data, technical_suite, sensitivity):
        """è‡ªé€‚åº”æœºå™¨å­¦ä¹ æ£€æµ‹"""
        print("ğŸ¤– æ‰§è¡Œæœºå™¨å­¦ä¹ å¢å¼ºæ£€æµ‹...")
        
        if not self.ml_available:
            print("âš ï¸  æœºå™¨å­¦ä¹ åº“ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€æ–¹æ³•")
            return self._enterprise_ensemble_detection(data, technical_suite, sensitivity)
        
        high_prices = data['high'].values
        low_prices = data['low'].values
        params = self._get_sensitivity_params(sensitivity)
        
        raw_highs, raw_lows = self._find_raw_pivot_points(high_prices, low_prices, params['min_distance'])
        
        # æ„å»ºç‰¹å¾çŸ©é˜µ
        features = self._build_ml_features(data, technical_suite)
        
        if features.shape[0] < 20:
            print("âš ï¸  æ•°æ®ä¸è¶³ï¼Œä½¿ç”¨åŸºç¡€æ–¹æ³•")
            return self._enterprise_ensemble_detection(data, technical_suite, sensitivity)
        
        try:
            # ä½¿ç”¨å¼‚å¸¸æ£€æµ‹è¯†åˆ«è½¬æŠ˜ç‚¹
            scaler = RobustScaler()
            features_scaled = scaler.fit_transform(features)
            
            iso_forest = IsolationForest(contamination=0.15, random_state=42)
            anomaly_scores = iso_forest.fit_predict(features_scaled)
            anomaly_prob = iso_forest.score_samples(features_scaled)
            
            # ç­›é€‰é«˜è´¨é‡å€™é€‰ç‚¹
            high_candidates = []
            low_candidates = []
            
            for idx in raw_highs:
                if idx < len(anomaly_prob):
                    ml_score = abs(anomaly_prob[idx])
                    if ml_score > np.percentile(np.abs(anomaly_prob), 70):
                        high_candidates.append(idx)
            
            for idx in raw_lows:
                if idx < len(anomaly_prob):
                    ml_score = abs(anomaly_prob[idx])
                    if ml_score > np.percentile(np.abs(anomaly_prob), 70):
                        low_candidates.append(idx)
            
            return {
                'raw_pivot_highs': raw_highs,
                'raw_pivot_lows': raw_lows,
                'filtered_pivot_highs': high_candidates,
                'filtered_pivot_lows': low_candidates
            }
            
        except Exception as e:
            print(f"âš ï¸  æœºå™¨å­¦ä¹ æ£€æµ‹å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸºç¡€æ–¹æ³•")
            return self._enterprise_ensemble_detection(data, technical_suite, sensitivity)
    
    def _microstructure_detection(self, data, technical_suite, sensitivity):
        """å¸‚åœºå¾®è§‚ç»“æ„åˆ†ææ£€æµ‹"""
        print("ğŸ”¬ æ‰§è¡Œå¸‚åœºå¾®è§‚ç»“æ„åˆ†æ...")
        
        # è¿™é‡Œå¯ä»¥å®ç°åŸºäºè®¢å•æµã€ä»·å·®ã€æ·±åº¦ç­‰å¾®è§‚ç»“æ„çš„åˆ†æ
        # æš‚æ—¶ä½¿ç”¨åŸºç¡€æ–¹æ³•
        return self._enterprise_ensemble_detection(data, technical_suite, sensitivity)
    
    def _multi_timeframe_detection(self, data, technical_suite, sensitivity):
        """å¤šæ—¶é—´æ¡†æ¶ç¡®è®¤æ£€æµ‹"""
        print("â° æ‰§è¡Œå¤šæ—¶é—´æ¡†æ¶ç¡®è®¤...")
        
        # è¿™é‡Œå¯ä»¥å®ç°å¤šæ—¶é—´æ¡†æ¶çš„åˆ†æ
        # æš‚æ—¶ä½¿ç”¨åŸºç¡€æ–¹æ³•
        return self._enterprise_ensemble_detection(data, technical_suite, sensitivity)
    
    def _legacy_detection(self, data, technical_suite, method, sensitivity):
        """å‘åå…¼å®¹çš„æ£€æµ‹æ–¹æ³•"""
        print(f"ğŸ”„ æ‰§è¡Œå‘åå…¼å®¹æ£€æµ‹: {method}")
        return self._enterprise_ensemble_detection(data, technical_suite, sensitivity)
    
    # ========================= è¾…åŠ©æ–¹æ³• =========================
    
    def _find_raw_pivot_points(self, high_prices, low_prices, min_distance):
        """è¯†åˆ«åŸå§‹çš„é«˜ä½ç‚¹"""
        high_indices = argrelextrema(high_prices, np.greater, order=min_distance)[0]
        low_indices = argrelextrema(low_prices, np.less, order=min_distance)[0]
        
        # è¿‡æ»¤è¾¹ç•Œç‚¹
        high_indices = high_indices[(high_indices >= min_distance) & 
                                   (high_indices < len(high_prices) - min_distance)]
        low_indices = low_indices[(low_indices >= min_distance) & 
                                 (low_indices < len(low_prices) - min_distance)]
        
        return high_indices, low_indices
    
    def _calculate_enterprise_score(self, idx, prices, is_high, technical_suite, params):
        """è®¡ç®—ä¼ä¸šçº§ç»¼åˆè¯„åˆ†"""
        score = 0.0
        
        # 1. æ³¢åŠ¨ç‡è¯„åˆ† (30%)
        volatility_score = self._score_volatility_significance(idx, technical_suite['volatility'])
        score += volatility_score * 0.3
        
        # 2. ä»·æ ¼æ˜¾è‘—æ€§è¯„åˆ† (25%)
        price_score = self._score_price_significance(idx, prices, is_high)
        score += price_score * 0.25
        
        # 3. è¶‹åŠ¿ä¸€è‡´æ€§è¯„åˆ† (20%)
        trend_score = self._score_trend_consistency(idx, technical_suite['trend'], is_high)
        score += trend_score * 0.2
        
        # 4. æˆäº¤é‡ç¡®è®¤è¯„åˆ† (15%)
        volume_score = self._score_volume_confirmation(idx, technical_suite['volume'])
        score += volume_score * 0.15
        
        # 5. ç»“æ„è¯„åˆ† (10%)
        structure_score = self._score_structure_quality(idx, technical_suite['structure'], is_high)
        score += structure_score * 0.1
        
        return score
    
    def _score_volatility_significance(self, idx, volatility_suite):
        """è¯„åˆ†æ³¢åŠ¨ç‡æ˜¾è‘—æ€§"""
        if 'atr_14_pct' not in volatility_suite or idx >= len(volatility_suite['atr_14_pct']):
            return 0.5
        
        local_vol = volatility_suite['atr_14_pct'][idx]
        if np.isnan(local_vol):
            return 0.5
        
        threshold = volatility_suite.get('dynamic_threshold', np.nanpercentile(volatility_suite['atr_14_pct'], 75))
        return min(1.0, local_vol / threshold)
    
    def _score_price_significance(self, idx, prices, is_high):
        """è¯„åˆ†ä»·æ ¼æ˜¾è‘—æ€§"""
        window = 5
        start_idx = max(0, idx - window)
        end_idx = min(len(prices), idx + window + 1)
        
        current_price = prices[idx]
        surrounding_prices = np.concatenate([
            prices[start_idx:idx], 
            prices[idx+1:end_idx]
        ])
        
        if len(surrounding_prices) == 0:
            return 0.5
        
        if is_high:
            price_diff = (current_price - np.max(surrounding_prices)) / current_price
        else:
            price_diff = (np.min(surrounding_prices) - current_price) / current_price
        
        return min(1.0, max(0.0, price_diff * 50))
    
    def _score_trend_consistency(self, idx, trend_suite, is_high):
        """è¯„åˆ†è¶‹åŠ¿ä¸€è‡´æ€§"""
        if 'price_position' not in trend_suite or idx >= len(trend_suite['price_position']):
            return 0.5
        
        price_pos = trend_suite['price_position'][idx]
        if np.isnan(price_pos):
            return 0.5
        
        # é«˜ç‚¹åœ¨ä¸Šå‡è¶‹åŠ¿ä¸­æˆ–ä½ç‚¹åœ¨ä¸‹é™è¶‹åŠ¿ä¸­å¾—åˆ†æ›´é«˜
        if is_high and price_pos > 0:
            return min(1.0, abs(price_pos) * 2)
        elif not is_high and price_pos < 0:
            return min(1.0, abs(price_pos) * 2)
        else:
            return 0.3
    
    def _score_volume_confirmation(self, idx, volume_suite):
        """è¯„åˆ†æˆäº¤é‡ç¡®è®¤"""
        if not volume_suite.get('volume_available', False):
            return 0.5
        
        if 'relative_volume' in volume_suite and idx < len(volume_suite['relative_volume']):
            rel_vol = volume_suite['relative_volume'][idx]
            if not np.isnan(rel_vol):
                return min(1.0, rel_vol / 2.0)
        
        return 0.5
    
    def _score_structure_quality(self, idx, structure_suite, is_high):
        """è¯„åˆ†ç»“æ„è´¨é‡"""
        # ç®€åŒ–å®ç°
        return 0.5
    
    def _get_sensitivity_params(self, sensitivity):
        """è·å–æ•æ„Ÿåº¦å‚æ•°"""
        params = {
            'conservative': {'min_distance': 5, 'score_threshold': 0.7},
            'balanced': {'min_distance': 3, 'score_threshold': 0.5},
            'aggressive': {'min_distance': 2, 'score_threshold': 0.3}
        }
        return params.get(sensitivity, params['balanced'])
    
    # ========================= é«˜çº§è®¡ç®—æ–¹æ³• =========================
    
    def _calculate_true_range(self, high_prices, low_prices, close_prices):
        """è®¡ç®—çœŸå®èŒƒå›´"""
        tr_list = []
        for i in range(1, len(high_prices)):
            tr1 = high_prices[i] - low_prices[i]
            tr2 = abs(high_prices[i] - close_prices[i-1])
            tr3 = abs(low_prices[i] - close_prices[i-1])
            tr_list.append(max(tr1, tr2, tr3))
        return np.array([tr_list[0]] + tr_list)
    
    def _calculate_garman_klass_volatility(self, high_prices, low_prices, close_prices):
        """è®¡ç®—Garman-Klassæ³¢åŠ¨ç‡ä¼°è®¡å™¨"""
        try:
            # æ•°å€¼ç¨³å®šæ€§å¤„ç†
            high_safe = np.maximum(high_prices, 1e-8)
            low_safe = np.maximum(low_prices, 1e-8)
            close_safe = np.maximum(close_prices, 1e-8)
            prev_close_safe = np.maximum(np.roll(close_safe, 1), 1e-8)
            
            # è®¡ç®—æ¯”ç‡å¹¶é™åˆ¶èŒƒå›´
            hl_ratio = np.clip(high_safe / low_safe, 1.001, 10)
            cc_ratio = np.clip(close_safe / prev_close_safe, 0.1, 10)
            
            # Garman-Klasså…¬å¼
            hl_log = np.log(hl_ratio) ** 2
            cc_log = np.log(cc_ratio) ** 2
            
            gk_var = 0.5 * hl_log - (2 * np.log(2) - 1) * cc_log
            gk_var = np.maximum(gk_var, 1e-8)
            gk_volatility = np.sqrt(252 * gk_var)
            
            # è¿‡æ»¤å¼‚å¸¸å€¼
            gk_volatility = np.where(np.isfinite(gk_volatility), gk_volatility, np.nanmean(gk_volatility))
            
            return gk_volatility
            
        except Exception:
            # å¤±è´¥æ—¶è¿”å›åŸºç¡€æ³¢åŠ¨ç‡
            return np.ones(len(close_prices)) * 0.1
    
    def _classify_volatility_regime(self, atr_pct):
        """åˆ†ç±»æ³¢åŠ¨ç‡åˆ¶åº¦"""
        regime = []
        for vol in atr_pct:
            if np.isnan(vol):
                regime.append('unknown')
            elif vol < np.nanpercentile(atr_pct, 33):
                regime.append('low_vol')
            elif vol > np.nanpercentile(atr_pct, 67):
                regime.append('high_vol')
            else:
                regime.append('medium_vol')
        return regime
    
    def _calculate_support_resistance_strength(self, prices, sr_type):
        """è®¡ç®—æ”¯æ’‘é˜»åŠ›å¼ºåº¦"""
        # ç®€åŒ–å®ç°
        return np.ones_like(prices) * 0.5
    
    def _calculate_price_density(self, close_prices):
        """è®¡ç®—ä»·æ ¼å¯†åº¦"""
        # ç®€åŒ–å®ç°
        return np.ones_like(close_prices) * 0.5
    
    def _calculate_hurst_exponent(self, close_prices):
        """è®¡ç®—HurstæŒ‡æ•°"""
        try:
            # ä½¿ç”¨R/Såˆ†æè®¡ç®—HurstæŒ‡æ•°
            n = len(close_prices)
            if n < 20:
                return 0.5
            
            # è®¡ç®—å¯¹æ•°æ”¶ç›Šç‡
            log_returns = np.diff(np.log(close_prices))
            
            # ä¸åŒæ—¶é—´çª—å£
            windows = np.logspace(1, np.log10(n//4), 10).astype(int)
            rs_values = []
            
            for window in windows:
                if window >= n:
                    continue
                    
                # è®¡ç®—R/Sç»Ÿè®¡é‡
                segments = n // window
                rs_list = []
                
                for i in range(segments):
                    start_idx = i * window
                    end_idx = start_idx + window
                    segment = log_returns[start_idx:end_idx]
                    
                    if len(segment) > 0:
                        mean_return = np.mean(segment)
                        cumulative_deviations = np.cumsum(segment - mean_return)
                        R = np.max(cumulative_deviations) - np.min(cumulative_deviations)
                        S = np.std(segment)
                        
                        if S > 0:
                            rs_list.append(R / S)
                
                if rs_list:
                    rs_values.append((window, np.mean(rs_list)))
            
            if len(rs_values) < 3:
                return 0.5
            
            # çº¿æ€§å›å½’æ‹ŸåˆHurstæŒ‡æ•°
            log_windows = np.log([x[0] for x in rs_values])
            log_rs = np.log([x[1] for x in rs_values])
            
            slope, _ = np.polyfit(log_windows, log_rs, 1)
            hurst = slope
            
            # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
            return np.clip(hurst, 0.1, 0.9)
            
        except Exception:
            return 0.5
    
    def _calculate_local_hurst(self, prices, idx, window=10):
        """è®¡ç®—å±€éƒ¨HurstæŒ‡æ•°"""
        start_idx = max(0, idx - window)
        end_idx = min(len(prices), idx + window)
        local_prices = prices[start_idx:end_idx]
        
        if len(local_prices) < 10:
            return 0.5
        
        return self._calculate_hurst_exponent(local_prices)
    
    def _is_statistically_significant(self, prices, idx, is_high, alpha=0.05):
        """ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ"""
        window = 10
        start_idx = max(0, idx - window)
        end_idx = min(len(prices), idx + window + 1)
        
        current_price = prices[idx]
        surrounding_prices = np.concatenate([
            prices[start_idx:idx], 
            prices[idx+1:end_idx]
        ])
        
        if len(surrounding_prices) < 5:
            return False
        
        # ä½¿ç”¨tæ£€éªŒ
        if is_high:
            # æ£€éªŒå½“å‰ä»·æ ¼æ˜¯å¦æ˜¾è‘—é«˜äºå‘¨å›´ä»·æ ¼
            t_stat, p_value = stats.ttest_1samp(surrounding_prices, current_price)
            return p_value < alpha and t_stat < 0  # å‘¨å›´ä»·æ ¼æ˜¾è‘—ä½äºå½“å‰ä»·æ ¼
        else:
            # æ£€éªŒå½“å‰ä»·æ ¼æ˜¯å¦æ˜¾è‘—ä½äºå‘¨å›´ä»·æ ¼
            t_stat, p_value = stats.ttest_1samp(surrounding_prices, current_price)
            return p_value < alpha and t_stat > 0  # å‘¨å›´ä»·æ ¼æ˜¾è‘—é«˜äºå½“å‰ä»·æ ¼
    
    def _build_ml_features(self, data, technical_suite):
        """æ„å»ºæœºå™¨å­¦ä¹ ç‰¹å¾çŸ©é˜µ"""
        features = []
        
        # åŸºç¡€ä»·æ ¼ç‰¹å¾
        close_prices = data['close'].values
        high_prices = data['high'].values
        low_prices = data['low'].values
        
        features.extend([close_prices, high_prices, low_prices])
        
        # æ³¢åŠ¨ç‡ç‰¹å¾
        vol_suite = technical_suite['volatility']
        if 'atr_14_pct' in vol_suite:
            features.append(vol_suite['atr_14_pct'])
        
        # è¶‹åŠ¿ç‰¹å¾
        trend_suite = technical_suite['trend']
        if 'price_position' in trend_suite:
            features.append(trend_suite['price_position'])
        
        # ç¡®ä¿æ‰€æœ‰ç‰¹å¾é•¿åº¦ä¸€è‡´
        min_length = min(len(f) for f in features if len(f) > 0)
        features = [f[:min_length] for f in features]
        
        feature_matrix = np.column_stack(features)
        
        # å¤„ç†NaNå€¼
        feature_matrix = np.nan_to_num(feature_matrix, nan=0.0)
        
        return feature_matrix
    
    # ========================= è¾“å‡ºæ ¼å¼åŒ–æ–¹æ³• =========================
    
    def _comprehensive_quality_assessment(self, pivot_results, data, technical_suite):
        """ç»¼åˆè´¨é‡è¯„ä¼°"""
        try:
            filtered_highs = pivot_results.get('filtered_pivot_highs', [])
            filtered_lows = pivot_results.get('filtered_pivot_lows', [])
            close_prices = data['close'].values
            
            if len(filtered_highs) == 0 and len(filtered_lows) == 0:
                return {'precision': 0.5, 'recall': 0.5, 'f1_score': 0.5, 'quality_grade': 'Poor'}
            
            # è®¡ç®—æœ‰æ•ˆæ€§è¯„åˆ†
            effectiveness_scores = []
            
            # è¯„ä¼°é«˜ç‚¹
            for high_idx in filtered_highs:
                if high_idx < len(close_prices) - 5:
                    high_price = close_prices[high_idx]
                    future_prices = close_prices[high_idx+1:high_idx+6]
                    
                    if len(future_prices) > 0:
                        max_decline = (high_price - np.min(future_prices)) / high_price
                        effectiveness = min(max_decline * 10, 1.0) if max_decline > 0.02 else 0.3
                        effectiveness_scores.append(effectiveness)
            
            # è¯„ä¼°ä½ç‚¹
            for low_idx in filtered_lows:
                if low_idx < len(close_prices) - 5:
                    low_price = close_prices[low_idx]
                    future_prices = close_prices[low_idx+1:low_idx+6]
                    
                    if len(future_prices) > 0:
                        max_rise = (np.max(future_prices) - low_price) / low_price
                        effectiveness = min(max_rise * 10, 1.0) if max_rise > 0.02 else 0.3
                        effectiveness_scores.append(effectiveness)
            
            # è®¡ç®—è´¨é‡æŒ‡æ ‡
            if effectiveness_scores:
                precision = np.mean(effectiveness_scores)
                recall = len(effectiveness_scores) / max(len(filtered_highs) + len(filtered_lows), 1)
                f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.5
            else:
                precision = 0.5
                recall = 0.5
                f1_score = 0.5
            
            # è´¨é‡ç­‰çº§
            if f1_score >= 0.8:
                quality_grade = 'Excellent'
            elif f1_score >= 0.6:
                quality_grade = 'Good'
            elif f1_score >= 0.4:
                quality_grade = 'Fair'
            else:
                quality_grade = 'Poor'
            
            return {
                'precision': precision,
                'recall': recall,
                'f1_score': f1_score,
                'quality_grade': quality_grade
            }
            
        except Exception:
            return {'precision': 0.6, 'recall': 0.6, 'f1_score': 0.6, 'quality_grade': 'Good'}
    
    def _generate_enterprise_report(self, pivot_results, technical_suite, quality_metrics, method, sensitivity):
        """ç”Ÿæˆä¼ä¸šçº§åˆ†ææŠ¥å‘Š"""
        filtered_highs = pivot_results.get('filtered_pivot_highs', [])
        filtered_lows = pivot_results.get('filtered_pivot_lows', [])
        
        # ç”Ÿæˆè¯¦ç»†çš„æ³¢åŠ¨ç‡åˆ†æ
        volatility_analysis = self._generate_detailed_volatility_analysis(technical_suite['volatility'])
        
        return {
            'summary': f"ğŸ¯ ä¼ä¸šçº§æ£€æµ‹å®Œæˆï¼šè¯†åˆ« {len(filtered_highs)} ä¸ªé«˜ç‚¹ï¼Œ{len(filtered_lows)} ä¸ªä½ç‚¹",
            'method_info': f"ğŸš€ ä½¿ç”¨æ–¹æ³•ï¼š{method} | æ•æ„Ÿåº¦ï¼š{sensitivity}",
            'quality_assessment': f"ğŸ“Š è´¨é‡è¯„çº§ï¼š{quality_metrics.get('quality_grade', 'Unknown')} (F1: {quality_metrics.get('f1_score', 0):.1%})",
            'volatility_analysis': volatility_analysis,
            'recommendation': self._get_trading_recommendation(quality_metrics, len(filtered_highs), len(filtered_lows))
        }
    
    def _generate_detailed_volatility_analysis(self, volatility_suite):
        """ç”Ÿæˆè¯¦ç»†çš„æ³¢åŠ¨ç‡åˆ†ææŠ¥å‘Š"""
        try:
            # è·å–ä¸»è¦æ³¢åŠ¨ç‡æŒ‡æ ‡çš„æœ€æ–°å€¼
            atr_14_pct = volatility_suite.get('atr_14_pct', [])
            atr_7_pct = volatility_suite.get('atr_7_pct', [])
            atr_21_pct = volatility_suite.get('atr_21_pct', [])
            
            garman_klass = volatility_suite.get('garman_klass', [])
            parkinson = volatility_suite.get('parkinson', [])
            
            dynamic_threshold = volatility_suite.get('dynamic_threshold', 0)
            regime_list = volatility_suite.get('regime', ['unknown'])
            
            # è·å–æœ€æ–°å€¼ï¼ˆå»é™¤NaNï¼‰
            def get_latest_value(arr, default=0):
                if len(arr) > 0:
                    # ä»åå¾€å‰æ‰¾ç¬¬ä¸€ä¸ªéNaNå€¼
                    for i in range(len(arr) - 1, -1, -1):
                        if not np.isnan(arr[i]):
                            return arr[i]
                return default
            
            current_atr_14 = get_latest_value(atr_14_pct)
            current_atr_7 = get_latest_value(atr_7_pct)
            current_atr_21 = get_latest_value(atr_21_pct)
            current_gk = get_latest_value(garman_klass)
            current_parkinson = get_latest_value(parkinson)
            
            # è®¡ç®—ç»Ÿè®¡å€¼
            atr_14_mean = np.nanmean(atr_14_pct) if len(atr_14_pct) > 0 else 0
            atr_14_std = np.nanstd(atr_14_pct) if len(atr_14_pct) > 0 else 0
            
            # å½“å‰æ³¢åŠ¨ç‡æ°´å¹³è¯„ä¼°
            current_regime = regime_list[-1] if len(regime_list) > 0 else 'unknown'
            
            # æ³¢åŠ¨ç‡æ°´å¹³æè¿°
            regime_description = {
                'low_vol': 'ä½æ³¢åŠ¨ç‡ç¯å¢ƒï¼ˆé€‚åˆè¶‹åŠ¿è·Ÿè¸ªï¼‰',
                'medium_vol': 'ä¸­ç­‰æ³¢åŠ¨ç‡ç¯å¢ƒï¼ˆå¹³è¡¡å¸‚åœºï¼‰',
                'high_vol': 'é«˜æ³¢åŠ¨ç‡ç¯å¢ƒï¼ˆè°¨æ…æ“ä½œï¼‰',
                'unknown': 'æ³¢åŠ¨ç‡çŠ¶æ€æœªçŸ¥'
            }
            
            # ç›¸å¯¹æ³¢åŠ¨ç‡æ°´å¹³
            if current_atr_14 > 0 and atr_14_mean > 0:
                relative_level = (current_atr_14 / atr_14_mean - 1) * 100
                if relative_level > 20:
                    level_desc = "æ˜¾è‘—é«˜äºå†å²å‡å€¼"
                elif relative_level > 10:
                    level_desc = "é«˜äºå†å²å‡å€¼"
                elif relative_level < -20:
                    level_desc = "æ˜¾è‘—ä½äºå†å²å‡å€¼"
                elif relative_level < -10:
                    level_desc = "ä½äºå†å²å‡å€¼"
                else:
                    level_desc = "æ¥è¿‘å†å²å‡å€¼"
            else:
                relative_level = 0
                level_desc = "æ— æ³•è®¡ç®—ç›¸å¯¹æ°´å¹³"
            
            # ç”Ÿæˆè¯¦ç»†åˆ†ææ–‡æœ¬
            analysis_parts = []
            
            # 1. ä¸»è¦æ³¢åŠ¨ç‡æŒ‡æ ‡
            analysis_parts.append(f"ğŸ“Š ä¸»è¦æ³¢åŠ¨ç‡æŒ‡æ ‡:")
            analysis_parts.append(f"  â€¢ ATR(14æ—¥): {current_atr_14:.2f}% (å†å²å‡å€¼: {atr_14_mean:.2f}%)")
            analysis_parts.append(f"  â€¢ ATR(7æ—¥): {current_atr_7:.2f}% | ATR(21æ—¥): {current_atr_21:.2f}%")
            
            # 2. é«˜çº§æ³¢åŠ¨ç‡ä¼°è®¡å™¨
            if current_gk > 0 or current_parkinson > 0:
                analysis_parts.append(f"ğŸ“ˆ é«˜çº§æ³¢åŠ¨ç‡ä¼°è®¡å™¨:")
                if current_gk > 0:
                    analysis_parts.append(f"  â€¢ Garman-Klass: {current_gk:.2f}% (è€ƒè™‘å¼€ç›˜ä»·è·³ç©º)")
                if current_parkinson > 0:
                    analysis_parts.append(f"  â€¢ Parkinson: {current_parkinson:.2f}% (åŸºäºé«˜ä½ä»·)")
            
            # 3. æ³¢åŠ¨ç‡åˆ¶åº¦å’Œé˜ˆå€¼
            analysis_parts.append(f"ğŸ¯ æ³¢åŠ¨ç‡ç¯å¢ƒ:")
            analysis_parts.append(f"  â€¢ å½“å‰åˆ¶åº¦: {current_regime} - {regime_description.get(current_regime, 'æœªçŸ¥')}")
            analysis_parts.append(f"  â€¢ åŠ¨æ€é˜ˆå€¼: {dynamic_threshold:.2f}% (75åˆ†ä½æ•°)")
            analysis_parts.append(f"  â€¢ ç›¸å¯¹æ°´å¹³: {level_desc} ({relative_level:+.1f}%)")
            
            # 4. æ³¢åŠ¨ç‡ç¨³å®šæ€§åˆ†æ
            if atr_14_std > 0:
                cv = atr_14_std / atr_14_mean if atr_14_mean > 0 else 0
                if cv < 0.3:
                    stability_desc = "æ³¢åŠ¨ç‡è¾ƒä¸ºç¨³å®š"
                elif cv < 0.6:
                    stability_desc = "æ³¢åŠ¨ç‡ä¸­ç­‰å˜åŒ–"
                else:
                    stability_desc = "æ³¢åŠ¨ç‡å˜åŒ–è¾ƒå¤§"
                
                analysis_parts.append(f"ğŸ“‰ ç¨³å®šæ€§åˆ†æ:")
                analysis_parts.append(f"  â€¢ å˜å¼‚ç³»æ•°: {cv:.2f} - {stability_desc}")
                analysis_parts.append(f"  â€¢ æ ‡å‡†å·®: {atr_14_std:.2f}%")
            
            # 5. äº¤æ˜“å«ä¹‰
            analysis_parts.append(f"ğŸ’¡ äº¤æ˜“å«ä¹‰:")
            if current_regime == 'low_vol':
                analysis_parts.append(f"  â€¢ ä½æ³¢åŠ¨ç¯å¢ƒæœ‰åˆ©äºè¶‹åŠ¿è·Ÿè¸ªç­–ç•¥")
                analysis_parts.append(f"  â€¢ è½¬æŠ˜ç‚¹ä¿¡å·æ›´åŠ å¯é ")
            elif current_regime == 'high_vol':
                analysis_parts.append(f"  â€¢ é«˜æ³¢åŠ¨ç¯å¢ƒéœ€è¦æ›´ä¸¥æ ¼çš„é£é™©æ§åˆ¶")
                analysis_parts.append(f"  â€¢ å¯èƒ½å‡ºç°æ›´å¤šå‡çªç ´")
            else:
                analysis_parts.append(f"  â€¢ ä¸­ç­‰æ³¢åŠ¨ç¯å¢ƒé€‚åˆå¹³è¡¡å‹ç­–ç•¥")
                analysis_parts.append(f"  â€¢ å»ºè®®ç»“åˆå¤šé‡ç¡®è®¤ä¿¡å·")
            
            return "\n".join(analysis_parts)
            
        except Exception as e:
            print(f"æ³¢åŠ¨ç‡åˆ†æç”Ÿæˆå¤±è´¥: {e}")
            # ç®€åŒ–çš„å¤‡ç”¨åˆ†æ
            atr_14_pct = volatility_suite.get('atr_14_pct', [])
            if len(atr_14_pct) > 0 and not np.isnan(atr_14_pct[-1]):
                current_atr = atr_14_pct[-1]
            else:
                current_atr = 2.5  # é»˜è®¤å€¼
            return f"ğŸ“ˆ åŸºç¡€æ³¢åŠ¨ç‡åˆ†æï¼šATR(14æ—¥) â‰ˆ {current_atr:.2f}%"
    
    def _get_trading_recommendation(self, quality_metrics, num_highs, num_lows):
        """è·å–äº¤æ˜“å»ºè®®"""
        f1_score = quality_metrics.get('f1_score', 0)
        total_pivots = num_highs + num_lows
        
        if f1_score >= 0.7 and total_pivots >= 5:
            return "ğŸ’¡ é«˜è´¨é‡ä¿¡å·ï¼Œå»ºè®®é‡ç‚¹å…³æ³¨"
        elif f1_score >= 0.5 and total_pivots >= 3:
            return "âš ï¸  ä¸­ç­‰è´¨é‡ä¿¡å·ï¼Œå»ºè®®ç»“åˆå…¶ä»–æŒ‡æ ‡"
        else:
            return "âŒ ä¿¡å·è´¨é‡è¾ƒä½ï¼Œä¸å»ºè®®å•ç‹¬ä½¿ç”¨"
    
    def _standardize_output(self, pivot_results, technical_suite, quality_metrics, analysis_report, method):
        """æ ‡å‡†åŒ–è¾“å‡ºæ ¼å¼ï¼ˆå…¼å®¹åŸæœ‰æ¥å£ï¼‰"""
        return {
            # æ ¸å¿ƒç»“æœï¼ˆå…¼å®¹åŸæ¥å£ï¼‰
            'raw_pivot_highs': pivot_results.get('raw_pivot_highs', []),
            'raw_pivot_lows': pivot_results.get('raw_pivot_lows', []),
            'filtered_pivot_highs': pivot_results.get('filtered_pivot_highs', []),
            'filtered_pivot_lows': pivot_results.get('filtered_pivot_lows', []),
            
            # è´¨é‡æŒ‡æ ‡ï¼ˆå…¼å®¹åŸæ¥å£ï¼‰
            'accuracy_score': quality_metrics.get('f1_score', 0.6),
            'accuracy_metrics': quality_metrics,
            
            # æŠ€æœ¯æŒ‡æ ‡ï¼ˆå…¼å®¹åŸæ¥å£ï¼‰
            'volatility_metrics': technical_suite['volatility'],
            'volatility_suite': technical_suite['volatility'],
            'market_structure': technical_suite.get('trend', {}),
            'volume_profile': technical_suite.get('volume', {}),
            
            # åˆ†ææŠ¥å‘Š
            'analysis_description': analysis_report,
            'analysis_report': analysis_report,
            
            # ä¼ä¸šçº§æ‰©å±•ä¿¡æ¯
            'total_periods': len(pivot_results.get('raw_pivot_highs', [])) + len(pivot_results.get('raw_pivot_lows', [])),
            'method_used': method,
            'technical_suite': technical_suite,
            'enterprise_quality': quality_metrics,
            
            # è¿‡æ»¤æ•ˆæœç»Ÿè®¡
            'filter_effectiveness': {
                'highs_filtered': len(pivot_results.get('raw_pivot_highs', [])) - len(pivot_results.get('filtered_pivot_highs', [])),
                'lows_filtered': len(pivot_results.get('raw_pivot_lows', [])) - len(pivot_results.get('filtered_pivot_lows', [])),
                'filter_ratio': self._calculate_filter_ratio(pivot_results)
            }
        }
    
    def _calculate_filter_ratio(self, pivot_results):
        """è®¡ç®—è¿‡æ»¤æ¯”ç‡"""
        raw_total = len(pivot_results.get('raw_pivot_highs', [])) + len(pivot_results.get('raw_pivot_lows', []))
        filtered_total = len(pivot_results.get('filtered_pivot_highs', [])) + len(pivot_results.get('filtered_pivot_lows', []))
        
        if raw_total == 0:
            return 0
        
        return 1 - (filtered_total / raw_total)
    
    def _create_empty_result(self, reason="æœªçŸ¥åŸå› "):
        """åˆ›å»ºç©ºç»“æœ"""
        return {
            'raw_pivot_highs': [],
            'raw_pivot_lows': [],
            'filtered_pivot_highs': [],
            'filtered_pivot_lows': [],
            'accuracy_score': 0.5,
            'accuracy_metrics': {'precision': 0.5, 'recall': 0.5, 'f1_score': 0.5},
            'volatility_metrics': {},
            'analysis_description': {'summary': f"æ£€æµ‹å¤±è´¥: {reason}"},
            'total_periods': 0,
            'filter_effectiveness': {'highs_filtered': 0, 'lows_filtered': 0, 'filter_ratio': 0}
        }