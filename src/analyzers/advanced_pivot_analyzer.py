# -*- coding: utf-8 -*-
"""
ä¼ä¸šçº§é«˜ä½ç‚¹åˆ†æå™¨ - èåˆé¡¶çº§é‡åŒ–äº¤æ˜“æŠ€æœ¯çš„æ™ºèƒ½è½¬æŠ˜ç‚¹è¯†åˆ«ç³»ç»Ÿ
é›†æˆåˆ†å½¢ç»´åº¦ã€å¤šæ—¶é—´æ¡†æ¶ã€ç»Ÿè®¡éªŒè¯ã€æœºå™¨å­¦ä¹ ã€å¸‚åœºå¾®è§‚ç»“æ„ç­‰å…ˆè¿›æŠ€æœ¯
"""

import numpy as np
import pandas as pd
from scipy import stats
from scipy.signal import argrelextrema, find_peaks
from scipy.optimize import minimize_scalar
import warnings
warnings.filterwarnings('ignore')

try:
    import talib
    TALIB_AVAILABLE = True
except ImportError:
    TALIB_AVAILABLE = False
    print("Warning: TA-Lib not available. Using numpy implementations.")

try:
    from sklearn.ensemble import IsolationForest, RandomForestClassifier
    from sklearn.preprocessing import StandardScaler, RobustScaler
    from sklearn.cluster import DBSCAN
    from sklearn.metrics import silhouette_score
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False
    print("Warning: Scikit-learn not available. ML features disabled.")


class EnterprisesPivotAnalyzer:
    """
    ä¼ä¸šçº§é«˜ä½ç‚¹åˆ†æå™¨

    ç”¨é€”:
    - æä¾›ç»Ÿä¸€çš„é«˜ä½ç‚¹è¯†åˆ«æ¥å£ï¼Œå½“å‰ä»…ä¿ç•™ zigzag_atr ä¿¡å·æ–¹æ³•ï¼Œäº§å‡ºäº¤æ˜“å‘˜å¯ç”¨çš„å…¥åœº/å‡ºåœºçº§åˆ«çš„è½¬æŠ˜ç‚¹ã€‚

    å®ç°æ–¹å¼:
    - é¢„å¤„ç†: æ•°æ®æ¸…æ´—/IQR å»å™ªã€OHLC åˆè§„æ ¡æ­£ã€ç¼ºå¤±å¡«è¡¥
    - æŠ€æœ¯å¥—ä»¶: è®¡ç®—æ³¢åŠ¨ç‡/è¶‹åŠ¿/åŠ¨é‡/æˆäº¤é‡/åˆ†å½¢ç­‰æŒ‡æ ‡ï¼Œä¾›æ£€æµ‹ä¸è´¨é‡è¯„ä¼°å¤ç”¨
    - è¯†åˆ«: ä½¿ç”¨ ZigZag + ATR è‡ªé€‚åº”é˜ˆå€¼ï¼Œæ—¶é—´é¡ºåºæ„å»ºäº¤æ›¿é«˜ä½ç‚¹ï¼Œæ”¯æŒâ€œåŒå‘æ›´æç«¯æ›¿æ¢â€å’Œæœ€å°Kçº¿é—´éš”çº¦æŸ
    - è¯„ä¼°: ç»Ÿä¸€ç”Ÿæˆè´¨é‡æŒ‡æ ‡/åˆ†ææŠ¥å‘Š/è¿‡æ»¤ç»Ÿè®¡ï¼Œå¹¶æ ‡å‡†åŒ–è¾“å‡ºç»“æ„ï¼›HTML å¯è¯»å– pivot_meta è§£é‡Šæ¯ä¸ªç‚¹å…¥é€‰åŸå› 

    ä¼˜ç‚¹:
    - ä½å»¶è¿Ÿã€å¯è§£é‡Šï¼ˆprominenceã€é˜ˆå€¼ã€ATR%ã€swing_pctï¼‰ï¼Œè´´è¿‘å®é™…äº¤æ˜“ä½¿ç”¨
    - é˜ˆå€¼éš ATR è‡ªé€‚åº”ï¼Œä¸åŒæ³¢åŠ¨ç¯å¢ƒä¸‹å…·æœ‰ç¨³å¥æ€§
    - è¾“å‡ºæ ¼å¼å‘åå…¼å®¹ï¼Œæ˜“äºä¸å›¾è¡¨/HTML å¤ç”¨

    å±€é™:
    - ä»å±å¯å‘å¼æ–¹æ³•ï¼Œå¯¹å™ªå£°/è·³ç©ºå’Œæç«¯è¡Œæƒ…éœ€è¦è°¨æ…ï¼›æ•æ„Ÿåº¦å‚æ•°éœ€ç»“åˆæ ‡çš„å¾®è°ƒ
    - æœªå¼•å…¥ç›‘ç£å­¦ä¹ æ ‡ç­¾æ ¡å‡†ï¼›ä¸åŒå¸‚åœº/å‘¨æœŸåº”å•ç‹¬å›æµ‹

    ç»´æŠ¤å»ºè®®:
    - ä»…åœ¨ _get_sensitivity_params ä¸­è°ƒæ•´é£æ ¼å‚æ•°ï¼Œä¿æŒæ¥å£ç¨³å®š
    - å¦‚éœ€æ‰©å±•æŒ‡æ ‡/å…ƒä¿¡æ¯ï¼Œä¼˜å…ˆåœ¨ pivot_meta ä¸­è¿½åŠ é”®ï¼Œé¿å…ç ´åä¸‹æ¸¸
    - å¯¹å¤–æš´éœ² detect_pivot_points çš„è¿”å›é”®åè¯·ä¿æŒä¸å˜

    å…³é”®æ¥å£:
    - detect_pivot_points(data, method='zigzag_atr', sensitivity) -> dict
    - é‡è¦è¿”å›é”®: raw_pivot_highs/lows, filtered_pivot_highs/lows, pivot_meta, accuracy_score, volatility_metrics

    ä¾èµ–:
    - å¯é€‰ TA-Lib/Sklearnï¼›ç¼ºå¤±æ—¶è‡ªåŠ¨é™çº§
    """
    
    def __init__(self):
        self.talib_available = TALIB_AVAILABLE
        self.ml_available = ML_AVAILABLE
        
        # é…ç½®å‚æ•°
        self.config = {
            'fractal_dimension_window': 20,
            'statistical_significance_level': 0.05,
            'volatility_regime_lookback': 50,
            'multi_timeframe_weights': [0.4, 0.3, 0.2, 0.1],  # æƒé‡åˆ†é…
            'ml_ensemble_size': 5,
            'microstructure_window': 10
        }
        
    def detect_pivot_points(self, data, method='zigzag_atr', sensitivity='balanced', frequency: str = 'weekly', **kwargs):
        """
        ç»Ÿä¸€çš„é«˜ä½ç‚¹æ£€æµ‹æ¥å£ - ä¼ä¸šçº§æ£€æµ‹ç³»ç»Ÿ
        
        Args:
            data: pandas DataFrameï¼ŒåŒ…å«OHLCVæ•°æ®
            method: str, æ£€æµ‹æ–¹æ³•ï¼ˆä»…æ”¯æŒ 'zigzag_atr'ï¼‰
            sensitivity: str, æ•æ„Ÿåº¦ ['conservative', 'balanced', 'aggressive']
            
        Returns:
            dict: æ ‡å‡†åŒ–çš„åˆ†æç»“æœï¼Œå…¼å®¹åŸæœ‰æ¥å£
        """
        if len(data) < 30:
            return self._create_empty_result("æ•°æ®é•¿åº¦ä¸è¶³")
            
        try:
            print(f"ğŸš€ å¯åŠ¨ä¼ä¸šçº§é«˜ä½ç‚¹æ£€æµ‹ç³»ç»Ÿ - æ–¹æ³•: {method}")
            
            # 1. æ•°æ®é¢„å¤„ç†å’Œè´¨é‡æ£€æŸ¥
            processed_data = self._preprocess_data(data)
            if processed_data is None:
                return self._create_empty_result("æ•°æ®é¢„å¤„ç†å¤±è´¥")
            
            # 2. è®¡ç®—å…¨æ–¹ä½æŠ€æœ¯æŒ‡æ ‡å¥—ä»¶
            technical_suite = self._calculate_technical_suite(processed_data)
            
            # 3. ç»Ÿä¸€ä½¿ç”¨ ZigZag+ATR æ–¹æ³•ï¼ˆå…¶ä½™æ–¹æ³•å·²ç§»é™¤ï¼‰
            if method != 'zigzag_atr':
                print(f"æç¤º: æ–¹æ³• {method} å·²åºŸå¼ƒï¼Œå·²è‡ªåŠ¨åˆ‡æ¢ä¸º zigzag_atr")
            pivot_results = self._zigzag_atr_detection(processed_data, technical_suite, sensitivity, frequency)
            
            # 4. è´¨é‡è¯„ä¼°å’ŒéªŒè¯
            quality_metrics = self._comprehensive_quality_assessment(pivot_results, processed_data, technical_suite)
            
            # 5. ç”Ÿæˆä¼ä¸šçº§æŠ¥å‘Š
            analysis_report = self._generate_enterprise_report(
                pivot_results, technical_suite, quality_metrics, method, sensitivity
            )
            
            # 6. è®¡ç®—â€œä¼˜è´¨â€è¯„ä¼°ï¼ˆåŸºäºæœ€ä½æ¢è½´ä½ç‚¹ä»¥æ¥çš„å¹´åŒ–æ³¢åŠ¨ç‡ä¸å¤æ™®ï¼‰
            premium_metrics = self._compute_premium_metrics(data, pivot_results, frequency=frequency)

            # 7. æ ‡å‡†åŒ–è¾“å‡ºæ ¼å¼ï¼ˆå…¼å®¹åŸæœ‰æ¥å£ï¼‰
            return self._standardize_output(
                pivot_results, technical_suite, quality_metrics, analysis_report, method, premium_metrics
            )
            
        except Exception as e:
            print(f"âŒ ä¼ä¸šçº§é«˜ä½ç‚¹æ£€æµ‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return self._create_empty_result(f"æ£€æµ‹å¤±è´¥: {str(e)}")
    
    # ä¸ºäº†å…¼å®¹æ€§ï¼Œä¿ç•™æ—§æ–¹æ³•å
    def detect_advanced_pivots(self, data, method='enterprise_ensemble', sensitivity='balanced'):
        """å…¼å®¹æ€§æ–¹æ³•ï¼Œè°ƒç”¨æ–°çš„ç»Ÿä¸€æ¥å£"""
        return self.detect_pivot_points(data, method, sensitivity)
    
    # ========================= æ ¸å¿ƒæ£€æµ‹æ–¹æ³• =========================
    
    def _preprocess_data(self, data):
        """ä¼ä¸šçº§æ•°æ®é¢„å¤„ç†å’Œè´¨é‡æ£€æŸ¥"""
        try:
            # ç¡®ä¿æ•°æ®å®Œæ•´æ€§
            required_cols = ['open', 'high', 'low', 'close']
            for col in required_cols:
                if col not in data.columns:
                    print(f"âŒ ç¼ºå°‘å¿…è¦åˆ—: {col}")
                    return None
            
            # æ•°æ®æ¸…æ´—
            processed = data.copy()
            
            # å¤„ç†å¼‚å¸¸å€¼ï¼ˆä½¿ç”¨IQRæ–¹æ³•ï¼‰
            for col in required_cols:
                Q1 = processed[col].quantile(0.25)
                Q3 = processed[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                processed[col] = processed[col].clip(lower_bound, upper_bound)
            
            # ç¡®ä¿OHLCé€»è¾‘æ­£ç¡®æ€§
            processed['high'] = processed[['open', 'high', 'low', 'close']].max(axis=1)
            processed['low'] = processed[['open', 'high', 'low', 'close']].min(axis=1)
            
            # å¡«å……ç¼ºå¤±å€¼
            processed = processed.fillna(method='ffill').fillna(method='bfill')
            
            return processed
            
        except Exception as e:
            print(f"âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}")
            return None
    
    def _calculate_technical_suite(self, data):
        """è®¡ç®—å…¨æ–¹ä½æŠ€æœ¯æŒ‡æ ‡å¥—ä»¶"""
        suite = {}
        
        high_prices = data['high'].values
        low_prices = data['low'].values
        close_prices = data['close'].values
        volume = data.get('volume', pd.Series(index=data.index, dtype=float)).values
        
        # 1. æ³¢åŠ¨ç‡æŒ‡æ ‡æ—
        suite['volatility'] = self._calculate_volatility_suite(high_prices, low_prices, close_prices)
        
        # 2. è¶‹åŠ¿æŒ‡æ ‡æ—
        suite['trend'] = self._calculate_trend_suite(high_prices, low_prices, close_prices)
        
        # 3. åŠ¨é‡æŒ‡æ ‡æ—
        suite['momentum'] = self._calculate_momentum_suite(high_prices, low_prices, close_prices)
        
        # 4. æˆäº¤é‡æŒ‡æ ‡æ—
        suite['volume'] = self._calculate_volume_suite(close_prices, volume)
        
        # 5. å¸‚åœºç»“æ„æŒ‡æ ‡
        suite['structure'] = self._calculate_structure_suite(high_prices, low_prices, close_prices)
        
        # 6. åˆ†å½¢å’Œç»Ÿè®¡æŒ‡æ ‡
        suite['fractal'] = self._calculate_fractal_suite(close_prices)
        
        return suite
    
    def _calculate_volatility_suite(self, high_prices, low_prices, close_prices):
        """è®¡ç®—é«˜çº§æ³¢åŠ¨ç‡æŒ‡æ ‡å¥—ä»¶"""
        volatility_suite = {}
        
        # 1. ATR å®¶æ—
        for period in [7, 14, 21]:
            if self.talib_available:
                try:
                    # ç¡®ä¿æ•°æ®ç±»å‹ä¸ºfloat64
                    high_float = np.array(high_prices, dtype=np.float64)
                    low_float = np.array(low_prices, dtype=np.float64)
                    close_float = np.array(close_prices, dtype=np.float64)
                    atr = talib.ATR(high_float, low_float, close_float, timeperiod=period)
                except Exception:
                    # å¦‚æœTA-Libå¤±è´¥ï¼Œä½¿ç”¨æ‰‹åŠ¨è®¡ç®—
                    tr = self._calculate_true_range(high_prices, low_prices, close_prices)
                    atr = pd.Series(tr).rolling(window=period).mean().values
            else:
                tr = self._calculate_true_range(high_prices, low_prices, close_prices)
                atr = pd.Series(tr).rolling(window=period).mean().values
            volatility_suite[f'atr_{period}'] = atr
            volatility_suite[f'atr_{period}_pct'] = (atr / close_prices) * 100
        
        # 2. é«˜çº§æ³¢åŠ¨ç‡ä¼°è®¡å™¨
        try:
            # Garman-Klass ä¼°è®¡å™¨
            gk_vol = self._calculate_garman_klass_volatility(high_prices, low_prices, close_prices)
            volatility_suite['garman_klass'] = gk_vol
            
            # Parkinson ä¼°è®¡å™¨
            parkinson_vol = np.sqrt(252 / (4 * np.log(2))) * np.sqrt(np.log(high_prices / low_prices) ** 2)
            volatility_suite['parkinson'] = parkinson_vol
            
        except Exception as e:
            print(f"é«˜çº§æ³¢åŠ¨ç‡è®¡ç®—è­¦å‘Š: {e}")
            # ä½¿ç”¨åŸºç¡€ATRä½œä¸ºåå¤‡
            volatility_suite['garman_klass'] = volatility_suite.get('atr_14_pct', np.ones(len(close_prices)) * 5)
            volatility_suite['parkinson'] = volatility_suite.get('atr_14_pct', np.ones(len(close_prices)) * 5)
        
        # 3. åŠ¨æ€é˜ˆå€¼
        volatility_suite['dynamic_threshold'] = np.nanpercentile(volatility_suite['atr_14_pct'], 75)
        
        # 4. æ³¢åŠ¨ç‡åˆ¶åº¦åˆ†ç±»
        volatility_suite['regime'] = self._classify_volatility_regime(volatility_suite['atr_14_pct'])
        
        return volatility_suite
    
    def _calculate_trend_suite(self, high_prices, low_prices, close_prices):
        """è®¡ç®—è¶‹åŠ¿æŒ‡æ ‡æ—"""
        trend_suite = {}
        
        # 1. ç§»åŠ¨å¹³å‡çº¿æ—
        for period in [5, 10, 20, 50]:
            if self.talib_available:
                try:
                    close_float = np.array(close_prices, dtype=np.float64)
                    ma = talib.SMA(close_float, timeperiod=period)
                except Exception:
                    ma = pd.Series(close_prices).rolling(window=period).mean().values
            else:
                ma = pd.Series(close_prices).rolling(window=period).mean().values
            trend_suite[f'ma_{period}'] = ma
        
        # 2. è¶‹åŠ¿å¼ºåº¦
        if self.talib_available:
            try:
                high_float = np.array(high_prices, dtype=np.float64)
                low_float = np.array(low_prices, dtype=np.float64)
                close_float = np.array(close_prices, dtype=np.float64)
                trend_suite['adx'] = talib.ADX(high_float, low_float, close_float, timeperiod=14)
            except Exception:
                # ADXè®¡ç®—å¤±è´¥ï¼Œè·³è¿‡
                pass
        
        # 3. ä»·æ ¼ç›¸å¯¹ä½ç½®
        trend_suite['price_position'] = (close_prices - trend_suite['ma_20']) / trend_suite['ma_20']
        
        return trend_suite
    
    def _calculate_momentum_suite(self, high_prices, low_prices, close_prices):
        """è®¡ç®—åŠ¨é‡æŒ‡æ ‡æ—"""
        momentum_suite = {}
        
        # 1. RSI
        if self.talib_available:
            try:
                close_float = np.array(close_prices, dtype=np.float64)
                momentum_suite['rsi'] = talib.RSI(close_float, timeperiod=14)
            except Exception:
                # RSIè®¡ç®—å¤±è´¥ï¼Œè·³è¿‡
                pass
        
        # 2. ä»·æ ¼åŠ¨é‡
        momentum_suite['momentum_5'] = (close_prices - np.roll(close_prices, 5)) / np.roll(close_prices, 5)
        momentum_suite['momentum_10'] = (close_prices - np.roll(close_prices, 10)) / np.roll(close_prices, 10)
        
        return momentum_suite
    
    def _calculate_volume_suite(self, close_prices, volume):
        """è®¡ç®—æˆäº¤é‡æŒ‡æ ‡æ—"""
        volume_suite = {}
        
        if np.any(volume > 0):
            volume_suite['volume_available'] = True
            volume_suite['volume_ma'] = pd.Series(volume).rolling(window=20).mean().values
            volume_suite['relative_volume'] = volume / volume_suite['volume_ma']
            
            if self.talib_available:
                try:
                    # ç¡®ä¿æ•°æ®ç±»å‹ä¸ºfloat64ï¼ŒTA-Libè¦æ±‚
                    close_prices_float = np.array(close_prices, dtype=np.float64)
                    volume_float = np.array(volume, dtype=np.float64)
                    volume_suite['obv'] = talib.OBV(close_prices_float, volume_float)
                except Exception as e:
                    print(f"âš ï¸  OBVè®¡ç®—å‡ºé”™ï¼Œè·³è¿‡: {e}")
                    # ä¸è®¾ç½®OBVï¼Œç»§ç»­å…¶ä»–è®¡ç®—
        else:
            volume_suite['volume_available'] = False
        
        return volume_suite
    
    def _calculate_structure_suite(self, high_prices, low_prices, close_prices):
        """è®¡ç®—å¸‚åœºç»“æ„æŒ‡æ ‡"""
        structure_suite = {}
        
        # 1. æ”¯æ’‘é˜»åŠ›å¼ºåº¦
        structure_suite['support_strength'] = self._calculate_support_resistance_strength(low_prices, 'support')
        structure_suite['resistance_strength'] = self._calculate_support_resistance_strength(high_prices, 'resistance')
        
        # 2. ä»·æ ¼å¯†åº¦
        structure_suite['price_density'] = self._calculate_price_density(close_prices)
        
        return structure_suite
    
    def _calculate_fractal_suite(self, close_prices):
        """è®¡ç®—åˆ†å½¢å’Œç»Ÿè®¡æŒ‡æ ‡"""
        fractal_suite = {}
        
        # 1. åˆ†å½¢ç»´åº¦ï¼ˆHurstæŒ‡æ•°ï¼‰
        fractal_suite['hurst_exponent'] = self._calculate_hurst_exponent(close_prices)
        
        # 2. åˆ†å½¢ç»´åº¦
        fractal_suite['fractal_dimension'] = 2 - fractal_suite['hurst_exponent']
        
        return fractal_suite
    
    # ========================= æ£€æµ‹ç­–ç•¥å®ç° =========================
    
    # å·²ç§»é™¤å…¶ä»–æ–¹æ³•å®ç°ï¼Œç»Ÿä¸€ä½¿ç”¨ _zigzag_atr_detection
    
    # å·²ç§»é™¤å…¶ä»–æ–¹æ³•å®ç°ï¼Œç»Ÿä¸€ä½¿ç”¨ _zigzag_atr_detection
    
    def _statistical_significance_detection(self, data, technical_suite, sensitivity, frequency: str = 'weekly'):
        """åŸºäºç»Ÿè®¡æ˜¾è‘—æ€§çš„æ£€æµ‹ï¼ˆå¢å¼ºç‰ˆï¼‰
        ç›®æ ‡ï¼šæ›´è´´è¿‘äº¤æ˜“å‘˜çš„â€œæ˜¾è‘—é«˜ä½ç‚¹â€è®¤çŸ¥ï¼Œçªå‡ºï¼š
        - çªå‡ºåº¦ï¼ˆprominenceï¼‰è¶³å¤Ÿé«˜ï¼ˆATR/æ³¢åŠ¨ç‡è‡ªé€‚åº”ï¼‰
        - ä¸ä¸¤ä¾§çª—å£ç›¸æ¯”æ˜¾è‘—ï¼ˆt-test/å‡å€¼å·®å¼‚ï¼‰
        - æœ‰æœ€å°æŒ¯å¹…ï¼ˆç™¾åˆ†æ¯”/ATRï¼‰
        - æœ‰åéªŒç¡®è®¤ï¼ˆä»·æ ¼åœ¨æ•°æ ¹Kçº¿å†…å‘åæ–¹å‘è¿è¡Œè¶³å¤Ÿå¹…åº¦ï¼‰
        - æ¢è½´ä¹‹é—´ä¿æŒæ—¶é—´åˆ†ç¦»ï¼Œé¿å…ç°‡æ‹¥
        """
        print("ğŸ“ˆ æ‰§è¡Œç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒï¼ˆå¢å¼ºç‰ˆï¼‰...")

        close_prices = data['close'].values
        high_prices = data['high'].values
        low_prices = data['low'].values
        params = self._get_sensitivity_params(sensitivity, frequency)
        
        # è®¡ç®—ATRä»·æ ¼å°ºåº¦ï¼ˆç”¨äº prominence & ç¡®è®¤ï¼‰
        atr_price = self._compute_atr_price_scale(close_prices, technical_suite.get('volatility', {}))
        # å…¨å±€æœ€å° prominenceï¼ˆä»·æ ¼å•ä½ï¼‰
        min_prom_price = np.nanmedian(atr_price) * params.get('min_prominence_atr', 1.0)
        min_swing_pct = params.get('min_swing_pct', 0.02)  # 2%

        # 1) é€šè¿‡ prominence åˆç­›å€™é€‰
        raw_highs, high_prom_map = self._find_peaks_with_prominence(
            high_prices, distance=params['min_distance'], min_prominence=min_prom_price
        )
        raw_lows, low_prom_map_neg = self._find_peaks_with_prominence(
            -low_prices, distance=params['min_distance'], min_prominence=min_prom_price
        )
        # ä½ç‚¹çš„ prominence ä»¥ä»·æ ¼æ­£æ•°è¡¨è¾¾
        low_prom_map = {k: float(v) for k, v in low_prom_map_neg.items()}
        # å°†ä½ç‚¹ç´¢å¼•è¿˜åŸ
        raw_lows = raw_lows

        # 2) ä¸¥æ ¼çš„ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒï¼ˆåŒä¾§çª—å£ï¼‰+ æœ€å°æŒ¯å¹…ï¼ˆç™¾åˆ†æ¯”ï¼‰
        filtered_highs = []
        filtered_lows = []
        left_win = right_win = max(5, params['min_distance'])

        pivot_meta_highs = {}
        pivot_meta_lows = {}
        
        for idx in raw_highs:
            passed, z_l, z_r = self._is_statistically_significant_bilateral(high_prices, idx, is_high=True,
                                                                            left=left_win, right=right_win)
            if passed:
                # æœ€å°æŒ¯å¹…ï¼šç›¸å¯¹ä¸¤ä¾§å‡å€¼å¢å¹…
                left_mean = np.mean(high_prices[max(0, idx - left_win):idx]) if idx > 0 else high_prices[idx]
                right_mean = np.mean(high_prices[idx+1:idx+1+right_win]) if idx < len(high_prices)-1 else high_prices[idx]
                ref_mean = max(left_mean, right_mean)
                swing_ok = ref_mean > 0 and (high_prices[idx] - ref_mean) / ref_mean >= min_swing_pct
                conf_pass, move_val = self._confirm_pivot_move(close_prices, idx, is_high=True,
                                                               confirm_bars=params.get('confirm_bars', 2),
                                                               min_move=np.nanmedian(atr_price) * params.get('confirm_atr', 0.8))
                if swing_ok and conf_pass:
                    filtered_highs.append(idx)
                    pivot_meta_highs[idx] = {
                        'prominence': float(high_prom_map.get(idx, 0.0)),
                        'confirm_move': float(move_val),
                        'z_left': float(z_l),
                        'z_right': float(z_r)
                    }
        
        for idx in raw_lows:
            passed, z_l, z_r = self._is_statistically_significant_bilateral(low_prices, idx, is_high=False,
                                                                            left=left_win, right=right_win)
            if passed:
                left_mean = np.mean(low_prices[max(0, idx - left_win):idx]) if idx > 0 else low_prices[idx]
                right_mean = np.mean(low_prices[idx+1:idx+1+right_win]) if idx < len(low_prices)-1 else low_prices[idx]
                ref_mean = min(left_mean, right_mean)
                swing_ok = ref_mean > 0 and (ref_mean - low_prices[idx]) / ref_mean >= min_swing_pct
                conf_pass, move_val = self._confirm_pivot_move(close_prices, idx, is_high=False,
                                                               confirm_bars=params.get('confirm_bars', 2),
                                                               min_move=np.nanmedian(atr_price) * params.get('confirm_atr', 0.8))
                if swing_ok and conf_pass:
                    filtered_lows.append(idx)
                    pivot_meta_lows[idx] = {
                        'prominence': float(low_prom_map.get(idx, 0.0)),
                        'confirm_move': float(move_val),
                        'z_left': float(z_l),
                        'z_right': float(z_r)
                    }

        # 3) æ¢è½´ä¹‹é—´ä¿æŒæœ€å°é—´éš”ï¼ˆé¿å…è¿‡å¯†ï¼‰
        filtered_highs = self._enforce_min_separation(filtered_highs, params.get('separation_bars', 3))
        filtered_lows = self._enforce_min_separation(filtered_lows, params.get('separation_bars', 3))

        # å…ƒä¿¡æ¯ï¼šç”¨äºHTMLå±•ç¤º
        meta = {
            'pivot_meta_highs': {int(k): v for k, v in pivot_meta_highs.items() if k in filtered_highs},
            'pivot_meta_lows': {int(k): v for k, v in pivot_meta_lows.items() if k in filtered_lows}
        }
        
        return {
            'raw_pivot_highs': raw_highs,
            'raw_pivot_lows': raw_lows,
            'filtered_pivot_highs': filtered_highs,
            'filtered_pivot_lows': filtered_lows,
            'pivot_meta': meta
        }

    # ======== è¾…åŠ©ï¼šç»Ÿè®¡å¼ºåŒ–ä¸äº¤æ˜“å‘˜è®¤çŸ¥è´´åˆ ========
    def _compute_atr_price_scale(self, close_prices: np.ndarray, vol_suite: dict) -> np.ndarray:
        """å°† ATR% è½¬æ¢ä¸ºä»·æ ¼å°ºåº¦ï¼ˆè‹¥å­˜åœ¨ï¼‰ï¼›å¦åˆ™ç”¨æ”¶ç›Šç‡æ³¢åŠ¨ç‡è¿‘ä¼¼ã€‚"""
        atr_pct = vol_suite.get('atr_14_pct')
        if isinstance(atr_pct, np.ndarray) and len(atr_pct) == len(close_prices):
            return (atr_pct / 100.0) * np.maximum(close_prices, 1e-8)
        # é€€åŒ–æ–¹æ¡ˆï¼šç”¨è¿‡å»20æœŸçš„å¯¹æ•°æ”¶ç›Šç‡æ ‡å‡†å·®è¿‘ä¼¼
        returns = np.diff(np.log(np.maximum(close_prices, 1e-8))) if len(close_prices) > 1 else np.array([0.0])
        vol = np.zeros_like(close_prices)
        if len(returns) > 5:
            for i in range(1, len(close_prices)):
                win = returns[max(1, i-20):i]
                vol[i] = np.std(win) * close_prices[i]
        return vol

    def _find_peaks_with_prominence(self, series: np.ndarray, distance: int, min_prominence: float):
        """åŸºäº prominence çš„å³°å€¼åˆç­›ï¼ˆé«˜ç‚¹ä¼ åŸåºåˆ—ï¼Œä½ç‚¹ä¼ è´Ÿåºåˆ—ï¼‰ã€‚

        è¿”å›:
            peaks(list[int]), prom_map(dict[idx->prominence_price])
        """
        try:
            peaks, props = find_peaks(series, distance=max(1, distance), prominence=max(1e-8, float(min_prominence)))
            prominences = props.get('prominences', np.array([]))
            prom_map = {}
            for i, p in enumerate(peaks):
                prom_map[int(p)] = float(prominences[i]) if i < len(prominences) else 0.0
            return peaks.tolist(), prom_map
        except Exception:
            # å›é€€åˆ°ç›¸å¯¹æå€¼ï¼ˆæ—  prominence ä¿¡æ¯ï¼‰
            pts = argrelextrema(series, np.greater, order=max(1, distance))[0].tolist()
            return pts, {}

    def _is_statistically_significant_bilateral(self, prices: np.ndarray, idx: int, is_high: bool,
                                                left: int, right: int, alpha: float = 0.05):
        """åŒä¾§çª—å£æ˜¾è‘—æ€§ï¼šå½“å‰ä»·éœ€æ˜¾è‘—é«˜äºï¼ˆä½äºï¼‰ä¸¤ä¾§çª—å£å‡å€¼ã€‚
        è¿”å›: (passed: bool, z_left: float, z_right: float)
        """
        start_l = max(0, idx - left)
        left_arr = prices[start_l:idx]
        right_arr = prices[idx+1:idx+1+right]
        if len(left_arr) < 3 or len(right_arr) < 3:
            return False, 0.0, 0.0
        cur = prices[idx]
        # ç®€åŒ–ï¼šå½“å‰ä»·ä¸ä¸¤ä¾§å‡å€¼/æ ‡å‡†å·®å¯¹æ¯”ï¼ˆç¨³å®šå¯é ä¸”é«˜æ•ˆï¼‰
        def z_side(side_arr, higher: bool):
            mean = np.mean(side_arr)
            std = np.std(side_arr) + 1e-8
            z = (cur - mean) / std
            return z
        z_l = z_side(left_arr, higher=is_high)
        z_r = z_side(right_arr, higher=is_high)
        passed = (z_l > 1.64 and z_r > 1.64) if is_high else (z_l < -1.64 and z_r < -1.64)
        return passed, float(z_l), float(z_r)

    def _confirm_pivot_move(self, close_prices: np.ndarray, idx: int, is_high: bool,
                             confirm_bars: int, min_move: float):
        """åéªŒç¡®è®¤ï¼šåœ¨ confirm_bars å†…ï¼Œä»·æ ¼**åå‘**ç§»åŠ¨è‡³å°‘ min_moveã€‚
        è¿”å›: (passed: bool, move_value: float)
        """
        end = min(len(close_prices), idx + 1 + max(1, confirm_bars))
        if end <= idx + 1:
            return False, 0.0
        window = close_prices[idx+1:end]
        if len(window) == 0:
            return False, 0.0
        if is_high:
            # é«˜ç‚¹ï¼šåç»­æœ‰è¶³å¤Ÿä¸‹è¡Œ
            drop = close_prices[idx] - np.min(window)
            return drop >= max(1e-8, float(min_move)), float(drop)
        else:
            # ä½ç‚¹ï¼šåç»­æœ‰è¶³å¤Ÿä¸Šè¡Œ
            rise = np.max(window) - close_prices[idx]
            return rise >= max(1e-8, float(min_move)), float(rise)

    def _enforce_min_separation(self, indices: list, min_sep: int) -> list:
        if not indices:
            return []
        indices = sorted(indices)
        kept = [indices[0]]
        for i in indices[1:]:
            if i - kept[-1] >= max(1, min_sep):
                kept.append(i)
        return kept
    
    # å·²ç§»é™¤å…¶ä»–æ–¹æ³•å®ç°ï¼Œç»Ÿä¸€ä½¿ç”¨ _zigzag_atr_detection
    
    # å·²ç§»é™¤å…¶ä»–æ–¹æ³•å®ç°ï¼Œç»Ÿä¸€ä½¿ç”¨ _zigzag_atr_detection
    
    # å·²ç§»é™¤å…¶ä»–æ–¹æ³•å®ç°ï¼Œç»Ÿä¸€ä½¿ç”¨ _zigzag_atr_detection
    
    # å·²ç§»é™¤å…¶ä»–æ–¹æ³•å®ç°ï¼Œç»Ÿä¸€ä½¿ç”¨ _zigzag_atr_detection
    
    # ========================= è¾…åŠ©æ–¹æ³• =========================
    
    def _find_raw_pivot_points(self, high_prices, low_prices, min_distance):
        """è¯†åˆ«åŸå§‹çš„é«˜ä½ç‚¹"""
        high_indices = argrelextrema(high_prices, np.greater, order=min_distance)[0]
        low_indices = argrelextrema(low_prices, np.less, order=min_distance)[0]
        
        # è¿‡æ»¤è¾¹ç•Œç‚¹
        high_indices = high_indices[(high_indices >= min_distance) & 
                                   (high_indices < len(high_prices) - min_distance)]
        low_indices = low_indices[(low_indices >= min_distance) & 
                                 (low_indices < len(low_prices) - min_distance)]
        
        return high_indices, low_indices
    
    def _calculate_enterprise_score(self, idx, prices, is_high, technical_suite, params):
        """è®¡ç®—ä¼ä¸šçº§ç»¼åˆè¯„åˆ†"""
        score = 0.0
        
        # 1. æ³¢åŠ¨ç‡è¯„åˆ† (30%)
        volatility_score = self._score_volatility_significance(idx, technical_suite['volatility'])
        score += volatility_score * 0.3
        
        # 2. ä»·æ ¼æ˜¾è‘—æ€§è¯„åˆ† (25%)
        price_score = self._score_price_significance(idx, prices, is_high)
        score += price_score * 0.25
        
        # 3. è¶‹åŠ¿ä¸€è‡´æ€§è¯„åˆ† (20%)
        trend_score = self._score_trend_consistency(idx, technical_suite['trend'], is_high)
        score += trend_score * 0.2
        
        # 4. æˆäº¤é‡ç¡®è®¤è¯„åˆ† (15%)
        volume_score = self._score_volume_confirmation(idx, technical_suite['volume'])
        score += volume_score * 0.15
        
        # 5. ç»“æ„è¯„åˆ† (10%)
        structure_score = self._score_structure_quality(idx, technical_suite['structure'], is_high)
        score += structure_score * 0.1
        
        return score
    
    def _score_volatility_significance(self, idx, volatility_suite):
        """è¯„åˆ†æ³¢åŠ¨ç‡æ˜¾è‘—æ€§"""
        if 'atr_14_pct' not in volatility_suite or idx >= len(volatility_suite['atr_14_pct']):
            return 0.5
        
        local_vol = volatility_suite['atr_14_pct'][idx]
        if np.isnan(local_vol):
            return 0.5
        
        threshold = volatility_suite.get('dynamic_threshold', np.nanpercentile(volatility_suite['atr_14_pct'], 75))
        return min(1.0, local_vol / threshold)
    
    def _score_price_significance(self, idx, prices, is_high):
        """è¯„åˆ†ä»·æ ¼æ˜¾è‘—æ€§"""
        window = 5
        start_idx = max(0, idx - window)
        end_idx = min(len(prices), idx + window + 1)
        
        current_price = prices[idx]
        surrounding_prices = np.concatenate([
            prices[start_idx:idx], 
            prices[idx+1:end_idx]
        ])
        
        if len(surrounding_prices) == 0:
            return 0.5
        
        if is_high:
            price_diff = (current_price - np.max(surrounding_prices)) / current_price
        else:
            price_diff = (np.min(surrounding_prices) - current_price) / current_price
        
        return min(1.0, max(0.0, price_diff * 50))
    
    def _score_trend_consistency(self, idx, trend_suite, is_high):
        """è¯„åˆ†è¶‹åŠ¿ä¸€è‡´æ€§"""
        if 'price_position' not in trend_suite or idx >= len(trend_suite['price_position']):
            return 0.5
        
        price_pos = trend_suite['price_position'][idx]
        if np.isnan(price_pos):
            return 0.5
        
        # é«˜ç‚¹åœ¨ä¸Šå‡è¶‹åŠ¿ä¸­æˆ–ä½ç‚¹åœ¨ä¸‹é™è¶‹åŠ¿ä¸­å¾—åˆ†æ›´é«˜
        if is_high and price_pos > 0:
            return min(1.0, abs(price_pos) * 2)
        elif not is_high and price_pos < 0:
            return min(1.0, abs(price_pos) * 2)
        else:
            return 0.3
    
    def _score_volume_confirmation(self, idx, volume_suite):
        """è¯„åˆ†æˆäº¤é‡ç¡®è®¤"""
        if not volume_suite.get('volume_available', False):
            return 0.5
        
        if 'relative_volume' in volume_suite and idx < len(volume_suite['relative_volume']):
            rel_vol = volume_suite['relative_volume'][idx]
            if not np.isnan(rel_vol):
                return min(1.0, rel_vol / 2.0)
        
        return 0.5
    
    def _score_structure_quality(self, idx, structure_suite, is_high):
        """è¯„åˆ†ç»“æ„è´¨é‡"""
        # ç®€åŒ–å®ç°
        return 0.5
    
    def _get_sensitivity_params(self, sensitivity: str, frequency: str = 'weekly'):
        """è·å–æ•æ„Ÿåº¦å‚æ•°ï¼ŒæŒ‰é¢‘ç‡ï¼ˆæ—¥/å‘¨ï¼‰æä¾›å·®å¼‚åŒ–é»˜è®¤å€¼ã€‚"""
        # å‘¨é¢‘é»˜è®¤å‚æ•°ï¼ˆå‘¨Kï¼‰
        weekly = {
            'conservative': {
                'min_distance': 5,
                'score_threshold': 0.7,
                'min_prominence_atr': 1.2,
                'min_swing_pct': 0.025,
                'confirm_bars': 3,
                'confirm_atr': 1.0,
                'separation_bars': 4,
                'zigzag_min_distance': 5,
                'zigzag_base_swing_pct': 0.035,   # 3.5%
                'zigzag_atr_mult': 0.80,
                'zigzag_min_bars_between': 2,
                'zigzag_prom_atr_mult': 0.80
            },
            'balanced': {
                'min_distance': 3,
                'score_threshold': 0.5,
                'min_prominence_atr': 1.0,
                'min_swing_pct': 0.020,
                'confirm_bars': 2,
                'confirm_atr': 0.8,
                'separation_bars': 3,
                'zigzag_min_distance': 3,
                'zigzag_base_swing_pct': 0.025,   # 2.5%
                'zigzag_atr_mult': 0.60,
                'zigzag_min_bars_between': 2,
                'zigzag_prom_atr_mult': 0.60
            },
            'aggressive': {
                'min_distance': 2,
                'score_threshold': 0.3,
                'min_prominence_atr': 0.8,
                'min_swing_pct': 0.015,
                'confirm_bars': 1,
                'confirm_atr': 0.6,
                'separation_bars': 2,
                'zigzag_min_distance': 2,
                'zigzag_base_swing_pct': 0.018,   # 1.8%
                'zigzag_atr_mult': 0.50,
                'zigzag_min_bars_between': 1,
                'zigzag_prom_atr_mult': 0.50
            }
        }

        # æ—¥é¢‘è¦†ç›–ï¼ˆè¿‘3ä¸ªæœˆï¼‰
        daily_overrides = {
            'conservative': {
                'zigzag_min_distance': 3,
                'zigzag_base_swing_pct': 0.018,  # 1.8%
                'zigzag_atr_mult': 0.80,
                'zigzag_min_bars_between': 3,
                'zigzag_prom_atr_mult': 1.00,
                'min_distance': 3
            },
            'balanced': {
                'zigzag_min_distance': 2,
                'zigzag_base_swing_pct': 0.012,  # 1.2%
                'zigzag_atr_mult': 0.60,
                'zigzag_min_bars_between': 2,
                'zigzag_prom_atr_mult': 0.80,
                'min_distance': 2
            },
            'aggressive': {
                'zigzag_min_distance': 2,
                'zigzag_base_swing_pct': 0.008,  # 0.8%
                'zigzag_atr_mult': 0.50,
                'zigzag_min_bars_between': 1,
                'zigzag_prom_atr_mult': 0.60,
                'min_distance': 2
            }
        }

        base = weekly.get(sensitivity, weekly['balanced']).copy()
        if str(frequency).lower() == 'daily':
            override = daily_overrides.get(sensitivity, {})
            base.update(override)
        return base

    def _zigzag_atr_detection(self, data, technical_suite, sensitivity, frequency: str = 'weekly'):
        """ZigZag + ATR è‡ªé€‚åº”é˜ˆå€¼çš„ä¿¡å·æ¨¡å¼ï¼ˆæ›´è´´è¿‘äº¤æ˜“å‘˜è¿›å‡ºåœºï¼‰ã€‚

        ç‰¹å¾ï¼š
        - æ›´å®½æ¾çš„æ‘†åŠ¨é˜ˆå€¼ï¼šthreshold_pct = max(base_swing_pct, ATR_pct * atr_mult)
        - ä½¿ç”¨ prominence åˆç­›å€™é€‰ï¼ŒéšåæŒ‰æ—¶é—´é¡ºåºäº¤æ›¿æ„å»º pivots
        - ä¿ç•™æœ€æç«¯ç‚¹ï¼ˆåŒå‘å€™é€‰åªä¿ç•™æ›´é«˜/æ›´ä½è€…ï¼‰
        - é™åˆ¶ç›¸é‚» pivot çš„æœ€å°Kçº¿é—´éš”ï¼Œå‡å°‘æŠ–åŠ¨
        """

        close_prices = data['close'].values
        high_prices = data['high'].values
        low_prices = data['low'].values
        params = self._get_sensitivity_params(sensitivity, frequency)

        # ä»·æ ¼å°ºåº¦ä¸é˜ˆå€¼
        vol_suite = technical_suite.get('volatility', {})
        atr_pct_arr = vol_suite.get('atr_14_pct')
        if not isinstance(atr_pct_arr, np.ndarray) or len(atr_pct_arr) != len(close_prices):
            atr_pct_arr = np.zeros_like(close_prices) + 2.0  # é€€åŒ–ï¼šå‡è®¾2%

        base_swing = float(params.get('zigzag_base_swing_pct', 0.015))  # å°å¹…æ‘†åŠ¨
        atr_mult = float(params.get('zigzag_atr_mult', 0.6))
        min_bars_between = int(params.get('zigzag_min_bars_between', 2))
        prom_atr_mult = float(params.get('zigzag_prom_atr_mult', 0.6))
        min_distance = int(params.get('zigzag_min_distance', 2))

        # prominence åˆç­›
        atr_price = self._compute_atr_price_scale(close_prices, vol_suite)
        min_prom_price = np.nanmedian(atr_price) * prom_atr_mult
        high_candidates, high_prom_map = self._find_peaks_with_prominence(high_prices, distance=min_distance,
                                                                          min_prominence=min_prom_price)
        low_candidates, low_prom_map_neg = self._find_peaks_with_prominence(-low_prices, distance=min_distance,
                                                                            min_prominence=min_prom_price)
        low_prom_map = {k: float(v) for k, v in low_prom_map_neg.items()}

        # åˆå¹¶å€™é€‰å¹¶æŒ‰æ—¶é—´æ’åºï¼Œæ ‡æ³¨ç±»å‹ä¸ä»·æ ¼
        events = []
        for idx in high_candidates:
            if 0 <= idx < len(high_prices):
                events.append((idx, 'high', float(high_prices[idx])))
        for idx in low_candidates:
            if 0 <= idx < len(low_prices):
                events.append((idx, 'low', float(low_prices[idx])))
        events.sort(key=lambda x: x[0])

        filtered_highs = []
        filtered_lows = []
        pivot_meta_highs = {}
        pivot_meta_lows = {}

        last_type = None
        last_idx = None
        last_price = None

        def dynamic_threshold_pct(i):
            return max(base_swing, (atr_pct_arr[i] / 100.0) * atr_mult)

        for idx, typ, price in events:
            # æœ€å°æŸ±é—´éš”çº¦æŸ
            if last_idx is not None and (idx - last_idx) < min_bars_between:
                # åŒå‘ä¿ç•™æ›´æç«¯è€…
                if last_type == typ:
                    if (typ == 'high' and price > last_price) or (typ == 'low' and price < last_price):
                        # æ›¿æ¢æœ€åä¸€ä¸ªpivot
                        if typ == 'high' and filtered_highs:
                            filtered_highs[-1] = idx
                            last_idx, last_price = idx, price
                        elif typ == 'low' and filtered_lows:
                            filtered_lows[-1] = idx
                            last_idx, last_price = idx, price
                    # å¦åˆ™å¿½ç•¥
                # å¼‚å‘ä½†å¤ªè¿‘åˆ™å¿½ç•¥
                continue

            if last_type is None:
                # ç¬¬ä¸€ä¸ªå€™é€‰ç›´æ¥æ¥å—ä¸ºèµ·å§‹pivot
                if typ == 'high':
                    filtered_highs.append(idx)
                    last_type, last_idx, last_price = 'high', idx, price
                    pivot_meta_highs[idx] = {
                        'prominence': float(high_prom_map.get(idx, 0.0)),
                        'threshold_pct': float(dynamic_threshold_pct(idx)),
                        'atr_pct': float(atr_pct_arr[idx])
                    }
                else:
                    filtered_lows.append(idx)
                    last_type, last_idx, last_price = 'low', idx, price
                    pivot_meta_lows[idx] = {
                        'prominence': float(low_prom_map.get(idx, 0.0)),
                        'threshold_pct': float(dynamic_threshold_pct(idx)),
                        'atr_pct': float(atr_pct_arr[idx])
                    }
                continue

            if typ == last_type:
                # åŒå‘ï¼šä»…åœ¨æ›´æç«¯æ—¶æ›¿æ¢
                if (typ == 'high' and price > last_price) or (typ == 'low' and price < last_price):
                    if typ == 'high' and filtered_highs:
                        filtered_highs[-1] = idx
                        last_idx, last_price = idx, price
                        pivot_meta_highs[idx] = {
                            'prominence': float(high_prom_map.get(idx, 0.0)),
                            'threshold_pct': float(dynamic_threshold_pct(idx)),
                            'atr_pct': float(atr_pct_arr[idx])
                        }
                    elif typ == 'low' and filtered_lows:
                        filtered_lows[-1] = idx
                        last_idx, last_price = idx, price
                        pivot_meta_lows[idx] = {
                            'prominence': float(low_prom_map.get(idx, 0.0)),
                            'threshold_pct': float(dynamic_threshold_pct(idx)),
                            'atr_pct': float(atr_pct_arr[idx])
                        }
                continue

            # å¼‚å‘ï¼šæ£€æŸ¥æ‘†åŠ¨å¹…åº¦æ˜¯å¦è¾¾åˆ°é˜ˆå€¼
            thr = dynamic_threshold_pct(idx)
            swing_pct = (abs(price - last_price) / max(1e-8, last_price))
            if swing_pct >= thr:
                if typ == 'high':
                    filtered_highs.append(idx)
                    pivot_meta_highs[idx] = {
                        'prominence': float(high_prom_map.get(idx, 0.0)),
                        'threshold_pct': float(thr),
                        'atr_pct': float(atr_pct_arr[idx]),
                        'swing_pct': float(swing_pct)
                    }
                else:
                    filtered_lows.append(idx)
                    pivot_meta_lows[idx] = {
                        'prominence': float(low_prom_map.get(idx, 0.0)),
                        'threshold_pct': float(thr),
                        'atr_pct': float(atr_pct_arr[idx]),
                        'swing_pct': float(swing_pct)
                    }
                last_type, last_idx, last_price = typ, idx, price
            # æœªè¾¾åˆ°é˜ˆå€¼åˆ™å¿½ç•¥ï¼ˆç»§ç»­ç­‰å¾…æ›´è¿œçš„æ‘†åŠ¨ï¼‰

        # è¾“å‡ºæ ¼å¼
        filtered_highs = self._enforce_min_separation(filtered_highs, max(1, min_bars_between))
        filtered_lows = self._enforce_min_separation(filtered_lows, max(1, min_bars_between))

        meta = {
            'pivot_meta_highs': {int(k): v for k, v in pivot_meta_highs.items() if k in filtered_highs},
            'pivot_meta_lows': {int(k): v for k, v in pivot_meta_lows.items() if k in filtered_lows}
        }

        return {
            'raw_pivot_highs': filtered_highs,
            'raw_pivot_lows': filtered_lows,
            'filtered_pivot_highs': filtered_highs,
            'filtered_pivot_lows': filtered_lows,
            'pivot_meta': meta
        }
    
    # ========================= é«˜çº§è®¡ç®—æ–¹æ³• =========================
    
    def _calculate_true_range(self, high_prices, low_prices, close_prices):
        """è®¡ç®—çœŸå®èŒƒå›´"""
        tr_list = []
        for i in range(1, len(high_prices)):
            tr1 = high_prices[i] - low_prices[i]
            tr2 = abs(high_prices[i] - close_prices[i-1])
            tr3 = abs(low_prices[i] - close_prices[i-1])
            tr_list.append(max(tr1, tr2, tr3))
        return np.array([tr_list[0]] + tr_list)
    
    def _calculate_garman_klass_volatility(self, high_prices, low_prices, close_prices):
        """è®¡ç®—Garman-Klassæ³¢åŠ¨ç‡ä¼°è®¡å™¨"""
        try:
            # æ•°å€¼ç¨³å®šæ€§å¤„ç†
            high_safe = np.maximum(high_prices, 1e-8)
            low_safe = np.maximum(low_prices, 1e-8)
            close_safe = np.maximum(close_prices, 1e-8)
            prev_close_safe = np.maximum(np.roll(close_safe, 1), 1e-8)
            
            # è®¡ç®—æ¯”ç‡å¹¶é™åˆ¶èŒƒå›´
            hl_ratio = np.clip(high_safe / low_safe, 1.001, 10)
            cc_ratio = np.clip(close_safe / prev_close_safe, 0.1, 10)
            
            # Garman-Klasså…¬å¼
            hl_log = np.log(hl_ratio) ** 2
            cc_log = np.log(cc_ratio) ** 2
            
            gk_var = 0.5 * hl_log - (2 * np.log(2) - 1) * cc_log
            gk_var = np.maximum(gk_var, 1e-8)
            gk_volatility = np.sqrt(252 * gk_var)
            
            # è¿‡æ»¤å¼‚å¸¸å€¼
            gk_volatility = np.where(np.isfinite(gk_volatility), gk_volatility, np.nanmean(gk_volatility))
            
            return gk_volatility
            
        except Exception:
            # å¤±è´¥æ—¶è¿”å›åŸºç¡€æ³¢åŠ¨ç‡
            return np.ones(len(close_prices)) * 0.1
    
    def _classify_volatility_regime(self, atr_pct):
        """åˆ†ç±»æ³¢åŠ¨ç‡åˆ¶åº¦"""
        regime = []
        for vol in atr_pct:
            if np.isnan(vol):
                regime.append('unknown')
            elif vol < np.nanpercentile(atr_pct, 33):
                regime.append('low_vol')
            elif vol > np.nanpercentile(atr_pct, 67):
                regime.append('high_vol')
            else:
                regime.append('medium_vol')
        return regime
    
    def _calculate_support_resistance_strength(self, prices, sr_type):
        """è®¡ç®—æ”¯æ’‘é˜»åŠ›å¼ºåº¦"""
        # ç®€åŒ–å®ç°
        return np.ones_like(prices) * 0.5
    
    def _calculate_price_density(self, close_prices):
        """è®¡ç®—ä»·æ ¼å¯†åº¦"""
        # ç®€åŒ–å®ç°
        return np.ones_like(close_prices) * 0.5
    
    def _calculate_hurst_exponent(self, close_prices):
        """è®¡ç®—HurstæŒ‡æ•°"""
        try:
            # ä½¿ç”¨R/Såˆ†æè®¡ç®—HurstæŒ‡æ•°
            n = len(close_prices)
            if n < 20:
                return 0.5
            
            # è®¡ç®—å¯¹æ•°æ”¶ç›Šç‡
            log_returns = np.diff(np.log(close_prices))
            
            # ä¸åŒæ—¶é—´çª—å£
            windows = np.logspace(1, np.log10(n//4), 10).astype(int)
            rs_values = []
            
            for window in windows:
                if window >= n:
                    continue
                    
                # è®¡ç®—R/Sç»Ÿè®¡é‡
                segments = n // window
                rs_list = []
                
                for i in range(segments):
                    start_idx = i * window
                    end_idx = start_idx + window
                    segment = log_returns[start_idx:end_idx]
                    
                    if len(segment) > 0:
                        mean_return = np.mean(segment)
                        cumulative_deviations = np.cumsum(segment - mean_return)
                        R = np.max(cumulative_deviations) - np.min(cumulative_deviations)
                        S = np.std(segment)
                        
                        if S > 0:
                            rs_list.append(R / S)
                
                if rs_list:
                    rs_values.append((window, np.mean(rs_list)))
            
            if len(rs_values) < 3:
                return 0.5
            
            # çº¿æ€§å›å½’æ‹ŸåˆHurstæŒ‡æ•°
            log_windows = np.log([x[0] for x in rs_values])
            log_rs = np.log([x[1] for x in rs_values])
            
            slope, _ = np.polyfit(log_windows, log_rs, 1)
            hurst = slope
            
            # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
            return np.clip(hurst, 0.1, 0.9)
            
        except Exception:
            return 0.5
    
    def _calculate_local_hurst(self, prices, idx, window=10):
        """è®¡ç®—å±€éƒ¨HurstæŒ‡æ•°"""
        start_idx = max(0, idx - window)
        end_idx = min(len(prices), idx + window)
        local_prices = prices[start_idx:end_idx]
        
        if len(local_prices) < 10:
            return 0.5
        
        return self._calculate_hurst_exponent(local_prices)
    
    def _is_statistically_significant(self, prices, idx, is_high, alpha=0.05):
        """ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ"""
        window = 10
        start_idx = max(0, idx - window)
        end_idx = min(len(prices), idx + window + 1)
        
        current_price = prices[idx]
        surrounding_prices = np.concatenate([
            prices[start_idx:idx], 
            prices[idx+1:end_idx]
        ])
        
        if len(surrounding_prices) < 5:
            return False
        
        # ä½¿ç”¨tæ£€éªŒ
        if is_high:
            # æ£€éªŒå½“å‰ä»·æ ¼æ˜¯å¦æ˜¾è‘—é«˜äºå‘¨å›´ä»·æ ¼
            t_stat, p_value = stats.ttest_1samp(surrounding_prices, current_price)
            return p_value < alpha and t_stat < 0  # å‘¨å›´ä»·æ ¼æ˜¾è‘—ä½äºå½“å‰ä»·æ ¼
        else:
            # æ£€éªŒå½“å‰ä»·æ ¼æ˜¯å¦æ˜¾è‘—ä½äºå‘¨å›´ä»·æ ¼
            t_stat, p_value = stats.ttest_1samp(surrounding_prices, current_price)
            return p_value < alpha and t_stat > 0  # å‘¨å›´ä»·æ ¼æ˜¾è‘—é«˜äºå½“å‰ä»·æ ¼
    
    def _build_ml_features(self, data, technical_suite):
        """æ„å»ºæœºå™¨å­¦ä¹ ç‰¹å¾çŸ©é˜µ"""
        features = []
        
        # åŸºç¡€ä»·æ ¼ç‰¹å¾
        close_prices = data['close'].values
        high_prices = data['high'].values
        low_prices = data['low'].values
        
        features.extend([close_prices, high_prices, low_prices])
        
        # æ³¢åŠ¨ç‡ç‰¹å¾
        vol_suite = technical_suite['volatility']
        if 'atr_14_pct' in vol_suite:
            features.append(vol_suite['atr_14_pct'])
        
        # è¶‹åŠ¿ç‰¹å¾
        trend_suite = technical_suite['trend']
        if 'price_position' in trend_suite:
            features.append(trend_suite['price_position'])
        
        # ç¡®ä¿æ‰€æœ‰ç‰¹å¾é•¿åº¦ä¸€è‡´
        min_length = min(len(f) for f in features if len(f) > 0)
        features = [f[:min_length] for f in features]
        
        feature_matrix = np.column_stack(features)
        
        # å¤„ç†NaNå€¼
        feature_matrix = np.nan_to_num(feature_matrix, nan=0.0)
        
        return feature_matrix
    
    # ========================= è¾“å‡ºæ ¼å¼åŒ–æ–¹æ³• =========================
    
    def _comprehensive_quality_assessment(self, pivot_results, data, technical_suite):
        """ç»¼åˆè´¨é‡è¯„ä¼°"""
        try:
            filtered_highs = pivot_results.get('filtered_pivot_highs', [])
            filtered_lows = pivot_results.get('filtered_pivot_lows', [])
            close_prices = data['close'].values
            
            if len(filtered_highs) == 0 and len(filtered_lows) == 0:
                return {'precision': 0.5, 'recall': 0.5, 'f1_score': 0.5, 'quality_grade': 'Poor'}
            
            # è®¡ç®—æœ‰æ•ˆæ€§è¯„åˆ†
            effectiveness_scores = []
            
            # è¯„ä¼°é«˜ç‚¹
            for high_idx in filtered_highs:
                if high_idx < len(close_prices) - 5:
                    high_price = close_prices[high_idx]
                    future_prices = close_prices[high_idx+1:high_idx+6]
                    
                    if len(future_prices) > 0:
                        max_decline = (high_price - np.min(future_prices)) / high_price
                        effectiveness = min(max_decline * 10, 1.0) if max_decline > 0.02 else 0.3
                        effectiveness_scores.append(effectiveness)
            
            # è¯„ä¼°ä½ç‚¹
            for low_idx in filtered_lows:
                if low_idx < len(close_prices) - 5:
                    low_price = close_prices[low_idx]
                    future_prices = close_prices[low_idx+1:low_idx+6]
                    
                    if len(future_prices) > 0:
                        max_rise = (np.max(future_prices) - low_price) / low_price
                        effectiveness = min(max_rise * 10, 1.0) if max_rise > 0.02 else 0.3
                        effectiveness_scores.append(effectiveness)
            
            # è®¡ç®—è´¨é‡æŒ‡æ ‡
            if effectiveness_scores:
                precision = np.mean(effectiveness_scores)
                recall = len(effectiveness_scores) / max(len(filtered_highs) + len(filtered_lows), 1)
                f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.5
            else:
                precision = 0.5
                recall = 0.5
                f1_score = 0.5
            
            # è´¨é‡ç­‰çº§
            if f1_score >= 0.8:
                quality_grade = 'Excellent'
            elif f1_score >= 0.6:
                quality_grade = 'Good'
            elif f1_score >= 0.4:
                quality_grade = 'Fair'
            else:
                quality_grade = 'Poor'
            
            return {
                'precision': precision,
                'recall': recall,
                'f1_score': f1_score,
                'quality_grade': quality_grade
            }
            
        except Exception:
            return {'precision': 0.6, 'recall': 0.6, 'f1_score': 0.6, 'quality_grade': 'Good'}
    
    def _generate_enterprise_report(self, pivot_results, technical_suite, quality_metrics, method, sensitivity):
        """ç”Ÿæˆä¼ä¸šçº§åˆ†ææŠ¥å‘Š"""
        filtered_highs = pivot_results.get('filtered_pivot_highs', [])
        filtered_lows = pivot_results.get('filtered_pivot_lows', [])
        
        # ç”Ÿæˆè¯¦ç»†çš„æ³¢åŠ¨ç‡åˆ†æ
        volatility_analysis = self._generate_detailed_volatility_analysis(technical_suite['volatility'])
        
        return {
            'summary': f"ğŸ¯ ä¼ä¸šçº§æ£€æµ‹å®Œæˆï¼šè¯†åˆ« {len(filtered_highs)} ä¸ªé«˜ç‚¹ï¼Œ{len(filtered_lows)} ä¸ªä½ç‚¹",
            'method_info': f"ğŸš€ ä½¿ç”¨æ–¹æ³•ï¼š{method} | æ•æ„Ÿåº¦ï¼š{sensitivity}",
            'quality_assessment': f"ğŸ“Š è´¨é‡è¯„çº§ï¼š{quality_metrics.get('quality_grade', 'Unknown')} (F1: {quality_metrics.get('f1_score', 0):.1%})",
            'volatility_analysis': volatility_analysis,
            'recommendation': self._get_trading_recommendation(quality_metrics, len(filtered_highs), len(filtered_lows))
        }
    
    def _generate_detailed_volatility_analysis(self, volatility_suite):
        """ç”Ÿæˆè¯¦ç»†çš„æ³¢åŠ¨ç‡åˆ†ææŠ¥å‘Š"""
        try:
            # è·å–ä¸»è¦æ³¢åŠ¨ç‡æŒ‡æ ‡çš„æœ€æ–°å€¼
            atr_14_pct = volatility_suite.get('atr_14_pct', [])
            atr_7_pct = volatility_suite.get('atr_7_pct', [])
            atr_21_pct = volatility_suite.get('atr_21_pct', [])
            
            garman_klass = volatility_suite.get('garman_klass', [])
            parkinson = volatility_suite.get('parkinson', [])
            
            dynamic_threshold = volatility_suite.get('dynamic_threshold', 0)
            regime_list = volatility_suite.get('regime', ['unknown'])
            
            # è·å–æœ€æ–°å€¼ï¼ˆå»é™¤NaNï¼‰
            def get_latest_value(arr, default=0):
                if len(arr) > 0:
                    # ä»åå¾€å‰æ‰¾ç¬¬ä¸€ä¸ªéNaNå€¼
                    for i in range(len(arr) - 1, -1, -1):
                        if not np.isnan(arr[i]):
                            return arr[i]
                return default
            
            current_atr_14 = get_latest_value(atr_14_pct)
            current_atr_7 = get_latest_value(atr_7_pct)
            current_atr_21 = get_latest_value(atr_21_pct)
            current_gk = get_latest_value(garman_klass)
            current_parkinson = get_latest_value(parkinson)
            
            # è®¡ç®—ç»Ÿè®¡å€¼
            atr_14_mean = np.nanmean(atr_14_pct) if len(atr_14_pct) > 0 else 0
            atr_14_std = np.nanstd(atr_14_pct) if len(atr_14_pct) > 0 else 0
            
            # å½“å‰æ³¢åŠ¨ç‡æ°´å¹³è¯„ä¼°
            current_regime = regime_list[-1] if len(regime_list) > 0 else 'unknown'
            
            # æ³¢åŠ¨ç‡æ°´å¹³æè¿°
            regime_description = {
                'low_vol': 'ä½æ³¢åŠ¨ç‡ç¯å¢ƒï¼ˆé€‚åˆè¶‹åŠ¿è·Ÿè¸ªï¼‰',
                'medium_vol': 'ä¸­ç­‰æ³¢åŠ¨ç‡ç¯å¢ƒï¼ˆå¹³è¡¡å¸‚åœºï¼‰',
                'high_vol': 'é«˜æ³¢åŠ¨ç‡ç¯å¢ƒï¼ˆè°¨æ…æ“ä½œï¼‰',
                'unknown': 'æ³¢åŠ¨ç‡çŠ¶æ€æœªçŸ¥'
            }
            
            # ç›¸å¯¹æ³¢åŠ¨ç‡æ°´å¹³
            if current_atr_14 > 0 and atr_14_mean > 0:
                relative_level = (current_atr_14 / atr_14_mean - 1) * 100
                if relative_level > 20:
                    level_desc = "æ˜¾è‘—é«˜äºå†å²å‡å€¼"
                elif relative_level > 10:
                    level_desc = "é«˜äºå†å²å‡å€¼"
                elif relative_level < -20:
                    level_desc = "æ˜¾è‘—ä½äºå†å²å‡å€¼"
                elif relative_level < -10:
                    level_desc = "ä½äºå†å²å‡å€¼"
                else:
                    level_desc = "æ¥è¿‘å†å²å‡å€¼"
            else:
                relative_level = 0
                level_desc = "æ— æ³•è®¡ç®—ç›¸å¯¹æ°´å¹³"
            
            # ç”Ÿæˆè¯¦ç»†åˆ†ææ–‡æœ¬
            analysis_parts = []
            
            # 1. ä¸»è¦æ³¢åŠ¨ç‡æŒ‡æ ‡
            analysis_parts.append(f"ğŸ“Š ä¸»è¦æ³¢åŠ¨ç‡æŒ‡æ ‡:")
            analysis_parts.append(f"  â€¢ ATR(14æ—¥): {current_atr_14:.2f}% (å†å²å‡å€¼: {atr_14_mean:.2f}%)")
            analysis_parts.append(f"  â€¢ ATR(7æ—¥): {current_atr_7:.2f}% | ATR(21æ—¥): {current_atr_21:.2f}%")
            
            # 2. é«˜çº§æ³¢åŠ¨ç‡ä¼°è®¡å™¨
            if current_gk > 0 or current_parkinson > 0:
                analysis_parts.append(f"ğŸ“ˆ é«˜çº§æ³¢åŠ¨ç‡ä¼°è®¡å™¨:")
                if current_gk > 0:
                    analysis_parts.append(f"  â€¢ Garman-Klass: {current_gk:.2f}% (è€ƒè™‘å¼€ç›˜ä»·è·³ç©º)")
                if current_parkinson > 0:
                    analysis_parts.append(f"  â€¢ Parkinson: {current_parkinson:.2f}% (åŸºäºé«˜ä½ä»·)")
            
            # 3. æ³¢åŠ¨ç‡åˆ¶åº¦å’Œé˜ˆå€¼
            analysis_parts.append(f"ğŸ¯ æ³¢åŠ¨ç‡ç¯å¢ƒ:")
            analysis_parts.append(f"  â€¢ å½“å‰åˆ¶åº¦: {current_regime} - {regime_description.get(current_regime, 'æœªçŸ¥')}")
            analysis_parts.append(f"  â€¢ åŠ¨æ€é˜ˆå€¼: {dynamic_threshold:.2f}% (75åˆ†ä½æ•°)")
            analysis_parts.append(f"  â€¢ ç›¸å¯¹æ°´å¹³: {level_desc} ({relative_level:+.1f}%)")
            
            # 4. æ³¢åŠ¨ç‡ç¨³å®šæ€§åˆ†æ
            if atr_14_std > 0:
                cv = atr_14_std / atr_14_mean if atr_14_mean > 0 else 0
                if cv < 0.3:
                    stability_desc = "æ³¢åŠ¨ç‡è¾ƒä¸ºç¨³å®š"
                elif cv < 0.6:
                    stability_desc = "æ³¢åŠ¨ç‡ä¸­ç­‰å˜åŒ–"
                else:
                    stability_desc = "æ³¢åŠ¨ç‡å˜åŒ–è¾ƒå¤§"
                
                analysis_parts.append(f"ğŸ“‰ ç¨³å®šæ€§åˆ†æ:")
                analysis_parts.append(f"  â€¢ å˜å¼‚ç³»æ•°: {cv:.2f} - {stability_desc}")
                analysis_parts.append(f"  â€¢ æ ‡å‡†å·®: {atr_14_std:.2f}%")
            
            # 5. äº¤æ˜“å«ä¹‰
            analysis_parts.append(f"ğŸ’¡ äº¤æ˜“å«ä¹‰:")
            if current_regime == 'low_vol':
                analysis_parts.append(f"  â€¢ ä½æ³¢åŠ¨ç¯å¢ƒæœ‰åˆ©äºè¶‹åŠ¿è·Ÿè¸ªç­–ç•¥")
                analysis_parts.append(f"  â€¢ è½¬æŠ˜ç‚¹ä¿¡å·æ›´åŠ å¯é ")
            elif current_regime == 'high_vol':
                analysis_parts.append(f"  â€¢ é«˜æ³¢åŠ¨ç¯å¢ƒéœ€è¦æ›´ä¸¥æ ¼çš„é£é™©æ§åˆ¶")
                analysis_parts.append(f"  â€¢ å¯èƒ½å‡ºç°æ›´å¤šå‡çªç ´")
            else:
                analysis_parts.append(f"  â€¢ ä¸­ç­‰æ³¢åŠ¨ç¯å¢ƒé€‚åˆå¹³è¡¡å‹ç­–ç•¥")
                analysis_parts.append(f"  â€¢ å»ºè®®ç»“åˆå¤šé‡ç¡®è®¤ä¿¡å·")
            
            return "\n".join(analysis_parts)
            
        except Exception as e:
            print(f"æ³¢åŠ¨ç‡åˆ†æç”Ÿæˆå¤±è´¥: {e}")
            # ç®€åŒ–çš„å¤‡ç”¨åˆ†æ
            atr_14_pct = volatility_suite.get('atr_14_pct', [])
            if len(atr_14_pct) > 0 and not np.isnan(atr_14_pct[-1]):
                current_atr = atr_14_pct[-1]
            else:
                current_atr = 2.5  # é»˜è®¤å€¼
            return f"ğŸ“ˆ åŸºç¡€æ³¢åŠ¨ç‡åˆ†æï¼šATR(14æ—¥) â‰ˆ {current_atr:.2f}%"
    
    def _get_trading_recommendation(self, quality_metrics, num_highs, num_lows):
        """è·å–äº¤æ˜“å»ºè®®"""
        f1_score = quality_metrics.get('f1_score', 0)
        total_pivots = num_highs + num_lows
        
        if f1_score >= 0.7 and total_pivots >= 5:
            return "ğŸ’¡ é«˜è´¨é‡ä¿¡å·ï¼Œå»ºè®®é‡ç‚¹å…³æ³¨"
        elif f1_score >= 0.5 and total_pivots >= 3:
            return "âš ï¸  ä¸­ç­‰è´¨é‡ä¿¡å·ï¼Œå»ºè®®ç»“åˆå…¶ä»–æŒ‡æ ‡"
        else:
            return "âŒ ä¿¡å·è´¨é‡è¾ƒä½ï¼Œä¸å»ºè®®å•ç‹¬ä½¿ç”¨"
    
    def _standardize_output(self, pivot_results, technical_suite, quality_metrics, analysis_report, method, premium_metrics=None):
        """æ ‡å‡†åŒ–è¾“å‡ºæ ¼å¼ï¼ˆå…¼å®¹åŸæœ‰æ¥å£ï¼‰"""
        return {
            # æ ¸å¿ƒç»“æœï¼ˆå…¼å®¹åŸæ¥å£ï¼‰
            'raw_pivot_highs': pivot_results.get('raw_pivot_highs', []),
            'raw_pivot_lows': pivot_results.get('raw_pivot_lows', []),
            'filtered_pivot_highs': pivot_results.get('filtered_pivot_highs', []),
            'filtered_pivot_lows': pivot_results.get('filtered_pivot_lows', []),
            
            # è´¨é‡æŒ‡æ ‡ï¼ˆå…¼å®¹åŸæ¥å£ï¼‰
            'accuracy_score': quality_metrics.get('f1_score', 0.6),
            'accuracy_metrics': quality_metrics,
            
            # æŠ€æœ¯æŒ‡æ ‡ï¼ˆå…¼å®¹åŸæ¥å£ï¼‰
            'volatility_metrics': technical_suite['volatility'],
            'volatility_suite': technical_suite['volatility'],
            'market_structure': technical_suite.get('trend', {}),
            'volume_profile': technical_suite.get('volume', {}),
            
            # åˆ†ææŠ¥å‘Š
            'analysis_description': analysis_report,
            'analysis_report': analysis_report,
            
            # ä¼ä¸šçº§æ‰©å±•ä¿¡æ¯
            'total_periods': len(pivot_results.get('raw_pivot_highs', [])) + len(pivot_results.get('raw_pivot_lows', [])),
            'method_used': method,
            'technical_suite': technical_suite,
            'enterprise_quality': quality_metrics,
            
            # ä¼˜è´¨è¯„ä¼°ï¼ˆæ–°å¢ï¼‰
            'premium_metrics': premium_metrics or {},
            
            # è¿‡æ»¤æ•ˆæœç»Ÿè®¡
            'filter_effectiveness': {
                'highs_filtered': len(pivot_results.get('raw_pivot_highs', [])) - len(pivot_results.get('filtered_pivot_highs', [])),
                'lows_filtered': len(pivot_results.get('raw_pivot_lows', [])) - len(pivot_results.get('filtered_pivot_lows', [])),
                'filter_ratio': self._calculate_filter_ratio(pivot_results)
            }
        }

    def _compute_premium_metrics(self, data, pivot_results, frequency: str = 'weekly'):
        """ä»è¯†åˆ«çš„ä½ç‚¹ä¸­æ‰¾æœ€ä½ç‚¹ï¼Œè®¡ç®—è‡ªè¯¥æ—¶ç‚¹è‡³ä»Šçš„å¹´åŒ–æ³¢åŠ¨ç‡ä¸å¤æ™®æ¯”ç‡ï¼Œå¹¶ç»™å‡ºâ€œä¼˜è´¨â€æ ‡æ³¨ã€‚

        è¿”å›:
            dict: {
                't1': str | None,                  # æœ€ä½ä½ç‚¹æ—¶é—´ï¼ˆISOå­—ç¬¦ä¸²ï¼‰
                'p1': float | None,                # æœ€ä½ä½ç‚¹ä»·æ ¼
                'annualized_volatility_pct': float,# å¹´åŒ–æ³¢åŠ¨ç‡ï¼ˆç™¾åˆ†æ¯”å€¼ï¼Œå¦‚ 45.2 è¡¨ç¤º45.2%ï¼‰
                'sharpe_ratio': float,             # å¤æ™®æ¯”ç‡ï¼ˆRf=0 å‡è®¾ï¼‰
                'is_premium': bool,                # æ˜¯å¦ä¼˜è´¨
                'reason': str                      # è¯´æ˜ï¼ˆå«R1/R2ï¼‰
            }
        """
        try:
            import numpy as np
            import pandas as pd

            filtered_lows = pivot_results.get('filtered_pivot_lows', []) or []
            if len(filtered_lows) == 0 or len(data) < 2:
                return {
                    't1': None,
                    'p1': None,
                    'annualized_volatility_pct': 0.0,
                    'sharpe_ratio': 0.0,
                    'is_premium': False,
                    'reason': 'æ— æœ‰æ•ˆä½ç‚¹æˆ–æ ·æœ¬è¿‡çŸ­'
                }

            low_prices = data['low'].values
            valid_idxs = [idx for idx in filtered_lows if 0 <= idx < len(low_prices)]
            if not valid_idxs:
                return {
                    't1': None,
                    'p1': None,
                    'annualized_volatility_pct': 0.0,
                    'sharpe_ratio': 0.0,
                    'is_premium': False,
                    'reason': 'ä½ç‚¹ç´¢å¼•æ— æ•ˆ'
                }

            # æ‰¾åˆ°è¯†åˆ«ä½ç‚¹ä¸­çš„æœ€ä½ç‚¹
            lowest_idx = min(valid_idxs, key=lambda i: low_prices[i])
            t1_ts = data.index[lowest_idx]
            try:
                t1_str = t1_ts.strftime('%Y-%m-%d') if hasattr(t1_ts, 'strftime') else str(t1_ts)
            except Exception:
                t1_str = str(t1_ts)
            p1_val = float(low_prices[lowest_idx])

            # è‡ªT1è‡³æœ€æ–°çš„å‘¨æ”¶ç›Šç‡åºåˆ—
            close_series = data['close'].iloc[lowest_idx:]
            if len(close_series) < 2:
                ann_vol_pct = 0.0
                sharpe = 0.0
            else:
                log_returns = np.diff(np.log(np.maximum(close_series.values, 1e-12)))
                if len(log_returns) == 0:
                    ann_vol_pct = 0.0
                    sharpe = 0.0
                else:
                    periods_per_year = 52 if str(frequency).lower() == 'weekly' else (252 if str(frequency).lower() == 'daily' else 52)
                    mu = float(np.mean(log_returns))
                    # ä½¿ç”¨æ— åä¼°è®¡ï¼ˆæ ·æœ¬æ ‡å‡†å·®ï¼‰
                    sigma = float(np.std(log_returns, ddof=1)) if len(log_returns) > 1 else float(np.std(log_returns))
                    ann_vol = sigma * np.sqrt(periods_per_year)
                    ann_vol_pct = float(ann_vol * 100.0)
                    sharpe = float((mu / sigma) * np.sqrt(periods_per_year)) if sigma > 1e-12 else 0.0

            # ä¼˜è´¨åˆ¤å®šé˜ˆå€¼ï¼ˆå¹´åŒ–æ³¢åŠ¨ç‡â‰¥40%ï¼Œå¤æ™®â‰¥0.8ï¼‰
            is_premium = (ann_vol_pct >= 40.0 and sharpe >= 0.8)
            reason = f"å¹´åŒ–æ³¢åŠ¨ç‡R1={ann_vol_pct:.1f}%ï¼Œå¤æ™®æ¯”ç‡R2={sharpe:.2f}"

            return {
                't1': t1_str,
                'p1': p1_val,
                'annualized_volatility_pct': ann_vol_pct,
                'sharpe_ratio': sharpe,
                'is_premium': bool(is_premium),
                'reason': reason
            }
        except Exception as e:
            return {
                't1': None,
                'p1': None,
                'annualized_volatility_pct': 0.0,
                'sharpe_ratio': 0.0,
                'is_premium': False,
                'reason': f'è®¡ç®—å¤±è´¥: {e}'
            }
    
    def _calculate_filter_ratio(self, pivot_results):
        """è®¡ç®—è¿‡æ»¤æ¯”ç‡"""
        raw_total = len(pivot_results.get('raw_pivot_highs', [])) + len(pivot_results.get('raw_pivot_lows', []))
        filtered_total = len(pivot_results.get('filtered_pivot_highs', [])) + len(pivot_results.get('filtered_pivot_lows', []))
        
        if raw_total == 0:
            return 0
        
        return 1 - (filtered_total / raw_total)
    
    def _create_empty_result(self, reason="æœªçŸ¥åŸå› "):
        """åˆ›å»ºç©ºç»“æœ"""
        return {
            'raw_pivot_highs': [],
            'raw_pivot_lows': [],
            'filtered_pivot_highs': [],
            'filtered_pivot_lows': [],
            'accuracy_score': 0.5,
            'accuracy_metrics': {'precision': 0.5, 'recall': 0.5, 'f1_score': 0.5},
            'volatility_metrics': {},
            'analysis_description': {'summary': f"æ£€æµ‹å¤±è´¥: {reason}"},
            'total_periods': 0,
            'filter_effectiveness': {'highs_filtered': 0, 'lows_filtered': 0, 'filter_ratio': 0}
        }