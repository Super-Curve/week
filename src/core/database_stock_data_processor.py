# -*- coding: utf-8 -*-
"""
æ•°æ®åº“ç‰ˆæœ¬çš„è‚¡ç¥¨æ•°æ®å¤„ç†å™¨
ä»MySQLæ•°æ®åº“è¯»å–è‚¡ç¥¨æ•°æ®å¹¶å¤„ç†ä¸ºå‘¨Kçº¿æ•°æ®
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import os
import pickle
import pymysql
from sqlalchemy import create_engine, text
import sys
import warnings
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from config.settings import DATABASE_CONFIG

warnings.filterwarnings('ignore')


class DatabaseStockDataProcessor:
    """
    æ•°æ®åº“è‚¡ç¥¨æ•°æ®å¤„ç†å™¨

    ç”¨é€”:
    - ä» MySQL è¯»å–å‘¨Kçº¿æ•°æ®ï¼Œæ”¯æŒé€‰æ‹©é›† selected_codesï¼›å¯¹ç»“æœåšæœ¬åœ°ç¼“å­˜ï¼ˆæŒ‰é€‰æ‹©é›†åˆ†æ¡¶ï¼‰ã€‚

    å®ç°æ–¹å¼:
    - SQLAlchemy è¿æ¥ï¼ˆè¿æ¥æ± ã€è¶…æ—¶é¢„è®¾ï¼‰â†’ é€ä»£ç æŸ¥è¯¢æœ€è¿‘ä¸‰å¹´å‘¨Kçº¿
    - ç¼“å­˜: cache/weekly_data_db.pkl æˆ–å¸¦ 8 ä½ md5 åç¼€çš„ç‹¬ç«‹ç¼“å­˜
    - å…¼å®¹æ¥å£: load_data/process_weekly_data/get_all_data/close_connection

    ä¼˜ç‚¹:
    - å®æ—¶æ€§å¥½ï¼›ç¼“å­˜å‡å°‘é‡å¤ IOï¼›é€‰æ‹©é›†åŠ é€Ÿä¸“é¢˜åˆ†æï¼ˆå¦‚ ARC Top 200ï¼‰

    å±€é™:
    - ä¾èµ–æ•°æ®åº“å¯ç”¨æ€§ï¼›schema å˜æ›´éœ€åŒæ­¥ SQL
    - å‘¨æœŸå›ºå®šä¸ºä¸‰å¹´ï¼Œè‹¥éœ€æ›´é•¿éœ€æš´éœ²å‚æ•°

    ç»´æŠ¤å»ºè®®:
    - æ›´æ–° _get_weekly_data_for_stock çš„ SQL ä¸ CAST ç²¾åº¦ä¸ç´¢å¼•
    - è°ƒæ•´ _is_cache_valid å¤±æ•ˆç­–ç•¥ï¼›å¿…è¦æ—¶å¼•å…¥å¢é‡æ›´æ–°
    - ä¸¥æ ¼ä¿æŒé”®ä¸ºè‚¡ç¥¨ä»£ç ã€å€¼ä¸º DataFrame çš„è¿”å›ç»“æ„
    """
    
    def __init__(self, cache_dir="cache", selected_codes=None):
        self.cache_dir = cache_dir
        self.weekly_data = {}
        self.daily_data = {}
        self.db_config = DATABASE_CONFIG
        self.engine = None
        # å¯é€‰ï¼šä»…åŠ è½½æŒ‡å®šè‚¡ç¥¨ä»£ç ï¼Œå‡å°‘æ•°æ®åº“ä¸å†…å­˜è´Ÿæ‹…
        self.selected_codes = list(selected_codes) if selected_codes else None
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        os.makedirs(self.cache_dir, exist_ok=True)
        
    def _get_cache_file(self):
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„ï¼ˆé’ˆå¯¹é€‰æ‹©é›†ç”Ÿæˆç‹¬ç«‹ç¼“å­˜ï¼‰ã€‚"""
        if self.selected_codes:
            # åŸºäºé€‰æ‹©é›†ç”Ÿæˆç®€å•çš„å“ˆå¸Œåç¼€ï¼Œé¿å…è¿‡é•¿æ–‡ä»¶å
            import hashlib
            join_key = ",".join(sorted(self.selected_codes))
            digest = hashlib.md5(join_key.encode("utf-8")).hexdigest()[:8]
            return os.path.join(self.cache_dir, f"weekly_data_db_{digest}.pkl")
        return os.path.join(self.cache_dir, "weekly_data_db.pkl")

    def _get_daily_cache_file(self, days: int = 90):
        """è·å–æ—¥çº¿ç¼“å­˜æ–‡ä»¶è·¯å¾„ï¼ˆæŒ‰é€‰æ‹©é›†ä¸daysåˆ†æ¡¶ï¼‰ã€‚"""
        if self.selected_codes:
            import hashlib
            join_key = ",".join(sorted(self.selected_codes))
            digest = hashlib.md5(join_key.encode("utf-8")).hexdigest()[:8]
            return os.path.join(self.cache_dir, f"daily_data_db_{days}_{digest}.pkl")
        return os.path.join(self.cache_dir, f"daily_data_db_{days}.pkl")
    
    def _get_cache_info_file(self):
        """è·å–ç¼“å­˜ä¿¡æ¯æ–‡ä»¶è·¯å¾„ï¼ˆé’ˆå¯¹é€‰æ‹©é›†ç”Ÿæˆç‹¬ç«‹ç¼“å­˜ä¿¡æ¯ï¼‰ã€‚"""
        if self.selected_codes:
            import hashlib
            join_key = ",".join(sorted(self.selected_codes))
            digest = hashlib.md5(join_key.encode("utf-8")).hexdigest()[:8]
            return os.path.join(self.cache_dir, f"cache_info_db_{digest}.txt")
        return os.path.join(self.cache_dir, "cache_info_db.txt")

    def _get_daily_cache_info_file(self, days: int = 90):
        """è·å–æ—¥çº¿ç¼“å­˜ä¿¡æ¯æ–‡ä»¶è·¯å¾„ã€‚"""
        if self.selected_codes:
            import hashlib
            join_key = ",".join(sorted(self.selected_codes))
            digest = hashlib.md5(join_key.encode("utf-8")).hexdigest()[:8]
            return os.path.join(self.cache_dir, f"cache_info_daily_db_{days}_{digest}.txt")
        return os.path.join(self.cache_dir, f"cache_info_daily_db_{days}.txt")
    
    def _is_cache_valid(self, max_age_hours=24):
        """
        æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        æ•°æ®åº“æ•°æ®åŸºäºæ—¶é—´å¤±æ•ˆï¼Œé»˜è®¤24å°æ—¶åé‡æ–°è·å–
        """
        cache_file = self._get_cache_file()
        info_file = self._get_cache_info_file()
        
        if not os.path.exists(cache_file) or not os.path.exists(info_file):
            return False
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
        cache_mtime = os.path.getmtime(cache_file)
        current_time = datetime.now().timestamp()
        
        # å¦‚æœç¼“å­˜è¶…è¿‡æŒ‡å®šå°æ—¶æ•°åˆ™è®¤ä¸ºè¿‡æœŸ
        return (current_time - cache_mtime) < max_age_hours * 3600

    def _is_daily_cache_valid(self, days: int = 90, max_age_hours: int = 12):
        """æ£€æŸ¥æ—¥çº¿ç¼“å­˜æ˜¯å¦æœ‰æ•ˆï¼ˆé»˜è®¤12å°æ—¶å¤±æ•ˆï¼‰ã€‚"""
        cache_file = self._get_daily_cache_file(days)
        info_file = self._get_daily_cache_info_file(days)
        if not os.path.exists(cache_file) or not os.path.exists(info_file):
            return False
        cache_mtime = os.path.getmtime(cache_file)
        current_time = datetime.now().timestamp()
        return (current_time - cache_mtime) < max_age_hours * 3600
    
    def _save_cache(self):
        """ä¿å­˜ç¼“å­˜"""
        cache_file = self._get_cache_file()
        info_file = self._get_cache_info_file()
        
        # ä¿å­˜æ•°æ®
        with open(cache_file, 'wb') as f:
            pickle.dump(self.weekly_data, f)
        
        # ä¿å­˜ç¼“å­˜ä¿¡æ¯
        with open(info_file, 'w', encoding='utf-8') as f:
            f.write("ç”Ÿæˆæ—¶é—´: {}\n".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
            f.write("æ•°æ®æº: æ•°æ®åº“\n")
            f.write("è‚¡ç¥¨æ•°é‡: {}\n".format(len(self.weekly_data)))
            f.write("æ•°æ®ç‚¹æ•°: {}\n".format(sum(len(data) for data in self.weekly_data.values())))
            if self.selected_codes:
                f.write("é€‰æ‹©é›†: {} åª (ç‹¬ç«‹ç¼“å­˜)\n".format(len(self.selected_codes)))
        
        print("ç¼“å­˜å·²ä¿å­˜åˆ°: {}".format(cache_file))

    def _save_daily_cache(self, days: int = 90):
        """ä¿å­˜æ—¥çº¿ç¼“å­˜ã€‚"""
        cache_file = self._get_daily_cache_file(days)
        info_file = self._get_daily_cache_info_file(days)
        with open(cache_file, 'wb') as f:
            pickle.dump(self.daily_data, f)
        with open(info_file, 'w', encoding='utf-8') as f:
            f.write("ç”Ÿæˆæ—¶é—´: {}\n".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
            f.write("æ•°æ®æº: æ•°æ®åº“ï¼ˆæ—¥çº¿ï¼‰\n")
            f.write("è‚¡ç¥¨æ•°é‡: {}\n".format(len(self.daily_data)))
            f.write("å¤©æ•°: {}\n".format(days))
            if self.selected_codes:
                f.write("é€‰æ‹©é›†: {} åª (ç‹¬ç«‹ç¼“å­˜)\n".format(len(self.selected_codes)))
        print("æ—¥çº¿ç¼“å­˜å·²ä¿å­˜åˆ°: {}".format(cache_file))
    
    def _load_cache(self):
        """åŠ è½½ç¼“å­˜"""
        cache_file = self._get_cache_file()
        
        try:
            with open(cache_file, 'rb') as f:
                self.weekly_data = pickle.load(f)
            print("ä»ç¼“å­˜åŠ è½½æ•°æ®: {} åªè‚¡ç¥¨".format(len(self.weekly_data)))
            return True
        except Exception as e:
            print("åŠ è½½ç¼“å­˜å¤±è´¥: {}".format(e))
            return False

    def _load_daily_cache(self, days: int = 90):
        """åŠ è½½æ—¥çº¿ç¼“å­˜ã€‚"""
        cache_file = self._get_daily_cache_file(days)
        try:
            with open(cache_file, 'rb') as f:
                self.daily_data = pickle.load(f)
            print("ä»æ—¥çº¿ç¼“å­˜åŠ è½½æ•°æ®: {} åªè‚¡ç¥¨".format(len(self.daily_data)))
            return True
        except Exception as e:
            print("åŠ è½½æ—¥çº¿ç¼“å­˜å¤±è´¥: {}".format(e))
            return False
    
    def _create_connection(self):
        """åˆ›å»ºæ•°æ®åº“è¿æ¥"""
        try:
            # åˆ›å»ºSQLAlchemyå¼•æ“
            connection_string = (
                f"mysql+pymysql://{self.db_config['username']}:{self.db_config['password']}"
                f"@{self.db_config['host']}:{self.db_config['port']}"
                f"/{self.db_config['database']}?charset={self.db_config['charset']}"
            )
            
            self.engine = create_engine(
                connection_string,
                pool_pre_ping=True,
                pool_recycle=3600,
                echo=False,
                # æ€§èƒ½ä¼˜åŒ–å‚æ•°
                pool_size=10,          # è¿æ¥æ± å¤§å°
                max_overflow=20,       # æœ€å¤§æº¢å‡ºè¿æ¥æ•°
                connect_args={
                    "connect_timeout": 60,     # è¿æ¥è¶…æ—¶
                    "read_timeout": 300,       # è¯»å–è¶…æ—¶
                    "write_timeout": 300,      # å†™å…¥è¶…æ—¶
                    "charset": "utf8mb4"
                }
            )
            
            # æµ‹è¯•è¿æ¥
            with self.engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            
            print("æ•°æ®åº“è¿æ¥æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            return False
    
    def _get_stock_codes(self):
        """è·å–è‚¡ç¥¨ä»£ç ï¼šè‹¥æœ‰é€‰æ‹©é›†ï¼Œåˆ™ç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™æŸ¥è¯¢å…¨é‡ã€‚"""
        if self.selected_codes:
            print(f"ä½¿ç”¨é€‰æ‹©é›†è‚¡ç¥¨ä»£ç  {len(self.selected_codes)} ä¸ª")
            return list(self.selected_codes)
        try:
            query = "SELECT stock_code FROM stock_name ORDER BY stock_code"
            df = pd.read_sql(query, self.engine)
            stock_codes = df['stock_code'].tolist()
            print(f"è·å–åˆ° {len(stock_codes)} ä¸ªè‚¡ç¥¨ä»£ç ")
            return stock_codes
        except Exception as e:
            print(f"è·å–è‚¡ç¥¨ä»£ç å¤±è´¥: {e}")
            return []
    
    def _get_weekly_data_for_stock(self, stock_code):
        """è·å–æŒ‡å®šè‚¡ç¥¨çš„å‘¨Kçº¿æ•°æ®"""
        try:
            # è®¡ç®—ä¸‰å¹´å‰çš„æ—¥æœŸ
            three_years_ago = (datetime.now() - timedelta(days=3*365)).strftime('%Y-%m-%d')
            
            # æŸ¥è¯¢SQL
            query = """
            SELECT 
                trade_date,
                CAST(open AS DECIMAL(10,2)) as open,
                CAST(high AS DECIMAL(10,2)) as high,
                CAST(low AS DECIMAL(10,2)) as low,
                CAST(close AS DECIMAL(10,2)) as close
            FROM history_week_data 
            WHERE code = %s AND trade_date >= %s
            ORDER BY trade_date
            """
            # æ‰§è¡ŒæŸ¥è¯¢
            df = pd.read_sql(query, self.engine, params=(stock_code, three_years_ago))

            
            if len(df) == 0:
                return None
            
            # æ•°æ®å¤„ç†
            df['trade_date'] = pd.to_datetime(df['trade_date'])
            df = df.set_index('trade_date')
            
            # æ•°æ®ç±»å‹è½¬æ¢
            numeric_cols = ['open', 'high', 'low', 'close']
            df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')
            
            # ç§»é™¤ç©ºå€¼å¹¶è¿”å›
            df = df.dropna()
            return df if len(df) > 0 else None
            
        except Exception:
            return None

    def _get_daily_data_for_stock(self, stock_code: str, days: int = 90):
        """è·å–æŒ‡å®šè‚¡ç¥¨çš„æœ€è¿‘Nå¤©æ—¥Kçº¿æ•°æ®ï¼ˆé»˜è®¤90å¤©ï¼ŒæŒ‰æ—¥å†å¤©ç­›é€‰ï¼‰ã€‚"""
        try:
            start_date = (datetime.now() - timedelta(days=days + 30)).strftime('%Y-%m-%d')
            query = """
            SELECT 
                trade_date,
                CAST(open AS DECIMAL(10,2)) as open,
                CAST(high AS DECIMAL(10,2)) as high,
                CAST(low AS DECIMAL(10,2)) as low,
                CAST(close AS DECIMAL(10,2)) as close
            FROM history_day_data 
            WHERE code = %s AND trade_date >= %s
            ORDER BY trade_date
            """
            df = pd.read_sql(query, self.engine, params=(stock_code, start_date))
            if len(df) == 0:
                return None
            df['trade_date'] = pd.to_datetime(df['trade_date'])
            df = df.set_index('trade_date')
            numeric_cols = ['open', 'high', 'low', 'close']
            df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')
            df = df.dropna()
            # åªä¿ç•™æœ€è¿‘daysä¸ªè‡ªç„¶æ—¥å†…çš„äº¤æ˜“è®°å½•
            cutoff = pd.Timestamp(datetime.now() - timedelta(days=days))
            df = df[df.index >= cutoff]
            return df if len(df) > 0 else None
        except Exception:
            return None
    
    def load_data(self):
        """
        å…¼å®¹æ€§æ–¹æ³•ï¼Œä¿æŒä¸åŸStockDataProcessoræ¥å£ä¸€è‡´
        å®é™…ä¸Šæ•°æ®åº“ç‰ˆæœ¬ä¸éœ€è¦è¿™ä¸ªæ–¹æ³•ï¼Œä½†ä¸ºäº†å…¼å®¹æ€§ä¿ç•™
        """
        return self._create_connection()
    
    def process_weekly_data(self):
        """å¤„ç†æ•°æ®ï¼Œä»æ•°æ®åº“è·å–å‘¨Kçº¿æ•°æ® - ç®€åŒ–ç‰ˆæœ¬"""
        # é¦–å…ˆå°è¯•ä»ç¼“å­˜åŠ è½½
        if self._is_cache_valid():
            if self._load_cache():
                return True
        
        # åˆ›å»ºæ•°æ®åº“è¿æ¥
        if not self._create_connection():
            print("æ— æ³•è¿æ¥åˆ°æ•°æ®åº“")
            return False
        
        print("å¼€å§‹ä»æ•°æ®åº“åŠ è½½å‘¨Kçº¿æ•°æ®...")
        
        try:
            # 1. è·å–æ‰€æœ‰è‚¡ç¥¨ä»£ç 
            stock_codes = self._get_stock_codes()
            if not stock_codes:
                print("æœªèƒ½è·å–è‚¡ç¥¨ä»£ç åˆ—è¡¨")
                return False
            
            print(f"è·å–åˆ° {len(stock_codes)} ä¸ªè‚¡ç¥¨ä»£ç ")
            
            # 2. éå†æ¯åªè‚¡ç¥¨è·å–æ•°æ®
            self._load_all_stock_data(stock_codes)
            
            print(f"æˆåŠŸå¤„ç† {len(self.weekly_data)} åªè‚¡ç¥¨çš„å‘¨Kçº¿æ•°æ®")
            
            # ä¿å­˜ç¼“å­˜
            if self.weekly_data:
                self._save_cache()
            
            return True
            
        except Exception as e:
            print(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return False

    def process_daily_data_recent(self, days: int = 90):
        """å¤„ç†æ•°æ®ï¼Œä»æ•°æ®åº“è·å–æœ€è¿‘Nå¤©ï¼ˆæ—¥çº¿ï¼‰æ•°æ®ã€‚"""
        if self._is_daily_cache_valid(days=days):
            if self._load_daily_cache(days=days):
                return True
        if not self._create_connection():
            print("æ— æ³•è¿æ¥åˆ°æ•°æ®åº“")
            return False
        print(f"å¼€å§‹ä»æ•°æ®åº“åŠ è½½æœ€è¿‘{days}å¤©çš„æ—¥Kçº¿æ•°æ®...")
        try:
            stock_codes = self._get_stock_codes()
            if not stock_codes:
                print("æœªèƒ½è·å–è‚¡ç¥¨ä»£ç åˆ—è¡¨")
                return False
            print(f"è·å–åˆ° {len(stock_codes)} ä¸ªè‚¡ç¥¨ä»£ç ")
            successful_count = 0
            total_count = len(stock_codes)
            for i, stock_code in enumerate(stock_codes, 1):
                if i % 100 == 0:
                    print(f"å¤„ç†è¿›åº¦(æ—¥): {i}/{total_count} ({i/total_count*100:.1f}%)")
                daily_df = self._get_daily_data_for_stock(stock_code, days=days)
                if daily_df is not None and len(daily_df) > 0:
                    self.daily_data[stock_code] = daily_df
                    successful_count += 1
            print(f"éå†å®Œæˆï¼ˆæ—¥çº¿ï¼‰ï¼ŒæˆåŠŸå¤„ç† {successful_count} åªè‚¡ç¥¨")
            if self.daily_data:
                self._save_daily_cache(days=days)
            return True
        except Exception as e:
            print(f"åŠ è½½æ—¥çº¿æ•°æ®å¤±è´¥: {e}")
            return False
    
    def _load_all_stock_data(self, stock_codes):
        """éå†æ‰€æœ‰è‚¡ç¥¨è·å–å‘¨Kçº¿æ•°æ® - ç®€åŒ–ç‰ˆæœ¬"""
        successful_count = 0
        total_count = len(stock_codes)
        
        print(f"å¼€å§‹éå† {total_count} åªè‚¡ç¥¨...")
        
        for i, stock_code in enumerate(stock_codes, 1):
            # æ¯100åªè‚¡ç¥¨æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
            if i % 100 == 0:
                print(f"å¤„ç†è¿›åº¦: {i}/{total_count} ({i/total_count*100:.1f}%)")
            
            # è·å–å•åªè‚¡ç¥¨çš„å‘¨Kçº¿æ•°æ®
            weekly_data = self._get_weekly_data_for_stock(stock_code)
            if weekly_data is not None and len(weekly_data) > 0:
                self.weekly_data[stock_code] = weekly_data
                successful_count += 1
        
        print(f"éå†å®Œæˆï¼ŒæˆåŠŸå¤„ç† {successful_count} åªè‚¡ç¥¨")
    
    def get_stock_codes(self):
        """è·å–æ‰€æœ‰è‚¡ç¥¨ä»£ç """
        return list(self.weekly_data.keys())
    
    def get_stock_data(self, code):
        """è·å–æŒ‡å®šè‚¡ç¥¨çš„å‘¨Kçº¿æ•°æ®"""
        return self.weekly_data.get(code)
    
    def get_all_data(self):
        """è·å–æ‰€æœ‰æ•°æ®"""
        return self.weekly_data

    def get_all_daily_data(self):
        """è·å–æ‰€æœ‰æ—¥çº¿æ•°æ®"""
        return self.daily_data
    
    def close_connection(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.engine:
            self.engine.dispose()
            print("æ•°æ®åº“è¿æ¥å·²å…³é—­")


if __name__ == "__main__":
    """æ€§èƒ½æµ‹è¯•"""
    import time
    start_time = time.time()
    
    processor = DatabaseStockDataProcessor()
    
    print("å¼€å§‹æ€§èƒ½æµ‹è¯•...")
    
    # æµ‹è¯•è¿æ¥
    if processor.load_data():
        print("æ•°æ®åº“è¿æ¥æˆåŠŸ")
        
        # æµ‹è¯•æ•°æ®å¤„ç†
        if processor.process_weekly_data():
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            stock_codes = processor.get_stock_codes()
            print(f"âœ… æˆåŠŸåŠ è½½ {len(stock_codes)} åªè‚¡ç¥¨çš„æ•°æ®")
            
            total_time = time.time() - start_time
            print(f"ğŸš€ æ€»è€—æ—¶: {total_time:.1f} ç§’")
            print(f"ğŸ“Š å¹³å‡æ¯åªè‚¡ç¥¨å¤„ç†æ—¶é—´: {total_time/len(stock_codes)*1000:.2f} æ¯«ç§’")
            
        else:
            print("âŒ æ•°æ®å¤„ç†å¤±è´¥")
    else:
        print("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥")
    
    processor.close_connection()