# -*- coding: utf-8 -*-
"""
æ•°æ®åº“ç‰ˆæœ¬çš„è‚¡ç¥¨æ•°æ®å¤„ç†å™¨
ä» MySQL è¯»å–å‘¨/æ—¥ K çº¿æ•°æ®ï¼Œå¹¶æä¾›ç»Ÿä¸€çš„è£…è½½ã€ç¼“å­˜ã€è®¿é—®æ¥å£ã€‚

- ä½¿ç”¨ SQLAlchemy ç®¡ç†è¿æ¥ä¸è¿æ¥æ± 
- å‘¨é¢‘é»˜è®¤è¿‘ä¸‰å¹´æ•°æ®ï¼›æ—¥é¢‘æŒ‰è‡ªç„¶æ—¥æˆªå–æœ€è¿‘ N å¤©
- æ‰€æœ‰å¯¹å¤–æ•°æ®ç»“æ„ä¿æŒä¸º {code: pandas.DataFrame}ï¼Œç´¢å¼•ä¸ºæ—¥æœŸï¼Œåˆ—ä¸º open/high/low/close
- æœ¬åœ°ç¼“å­˜æ”¾ç½®äº `cache/`ï¼Œå‘¨é¢‘ä¸æ—¥é¢‘åˆ†åˆ«åˆ†æ¡¶ï¼›å½“ä¼ å…¥é€‰æ‹©é›†æ—¶ï¼ŒæŒ‰é€‰æ‹©é›†ç”Ÿæˆç‹¬ç«‹ç¼“å­˜

æ³¨æ„ï¼šä»…è´Ÿè´£è¯»å–ä¸ç®€å•å¤„ç†ï¼ˆç±»å‹/ç´¢å¼•/ç¼ºå¤±å€¼ï¼‰ï¼Œä¸åšä¸šåŠ¡åˆ†æã€‚
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import os
import pickle
import pymysql
from sqlalchemy import create_engine, text
import sys
import warnings
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from config.settings import DATABASE_CONFIG

warnings.filterwarnings('ignore')


class DatabaseStockDataProcessor:
    """
    æ•°æ®åº“è‚¡ç¥¨æ•°æ®å¤„ç†å™¨

    ç”¨é€”:
    - ä» MySQL è¯»å–å‘¨/æ—¥ K çº¿æ•°æ®ï¼Œæ”¯æŒæŒ‰é€‰æ‹©é›†åŠ è½½ï¼Œç»“æœæ”¯æŒæœ¬åœ°ç¼“å­˜

    å®ç°:
    - è¿æ¥: SQLAlchemy å¼•æ“ï¼ˆé¢„ pingã€å›æ”¶ã€è¶…æ—¶ã€è¿æ¥æ± å‚æ•°ï¼‰
    - å‘¨é¢‘: æŸ¥è¯¢è¿‘ä¸‰å¹´å‘¨çº¿ï¼Œè¿”å› DataFrameï¼ˆç´¢å¼•ä¸º trade_dateï¼‰
    - æ—¥é¢‘: æŸ¥è¯¢æœ€è¿‘ N å¤©ï¼ˆæ—¥å†å¤©ï¼‰å†…çš„æ—¥çº¿
    - ç¼“å­˜: `cache/weekly_data_db*.pkl` ä¸ `cache/daily_data_db_*.pkl`ï¼Œå¹¶å†™å…¥ cache_info è¯´æ˜

    çº¦å®šä¸è¿”å›:
    - `load_data()` ä»…å»ºç«‹è¿æ¥ï¼›`process_weekly_data()`/`process_daily_data_recent()` å¡«å……å†…å­˜å­—å…¸
    - `get_all_data()`/`get_all_daily_data()` è¿”å›å†…å­˜ç¼“å­˜ï¼›é”®ä¸ºè‚¡ç¥¨ä»£ç ï¼Œå€¼ä¸º DataFrame

    ç»´æŠ¤å»ºè®®:
    - å¦‚æ•°æ®åº“è¡¨ç»“æ„è°ƒæ•´ï¼Œä¼˜å…ˆæ›´æ–° `_get_weekly_data_for_stock` ä¸ `_get_daily_data_for_stock` ä¸­çš„ SQL ä¸ CAST ç²¾åº¦
    - å¦‚éœ€å˜æ›´ç¼“å­˜ç­–ç•¥ï¼Œé›†ä¸­ä¿®æ”¹ `_is_cache_valid`/`_save_cache` åŠå¯¹åº”æ—¥é¢‘æ–¹æ³•
    - è¯·å‹¿æ›´æ”¹å¯¹å¤–è¿”å›å­—å…¸ç»“æ„ï¼Œé¿å…å½±å“ä¸Šæ¸¸åˆ†æ/æ¸²æŸ“
    """
    
    def __init__(self, cache_dir="cache", selected_codes=None):
        self.cache_dir = cache_dir
        self.weekly_data = {}
        self.daily_data = {}
        self.db_config = DATABASE_CONFIG
        self.engine = None
        # å¯é€‰ï¼šä»…åŠ è½½æŒ‡å®šè‚¡ç¥¨ä»£ç ï¼Œå‡å°‘æ•°æ®åº“ä¸å†…å­˜è´Ÿæ‹…
        self.selected_codes = list(selected_codes) if selected_codes else None
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        os.makedirs(self.cache_dir, exist_ok=True)

    def _get_cache_file(self):
        """è¿”å›å‘¨é¢‘ç¼“å­˜æ–‡ä»¶è·¯å¾„ã€‚å½“å­˜åœ¨é€‰æ‹©é›†æ—¶ï¼ŒåŸºäºé€‰æ‹©é›†å†…å®¹ç”Ÿæˆ md5 åç¼€ä»¥å®ç°ç‹¬ç«‹ç¼“å­˜ã€‚"""
        if self.selected_codes:
            import hashlib
            join_key = ",".join(sorted(self.selected_codes))
            digest = hashlib.md5(join_key.encode("utf-8")).hexdigest()[:8]
            return os.path.join(self.cache_dir, f"weekly_data_db_{digest}.pkl")
        return os.path.join(self.cache_dir, "weekly_data_db.pkl")

    def _get_daily_cache_file(self, days: int = 90):
        """è¿”å›æ—¥é¢‘ç¼“å­˜æ–‡ä»¶è·¯å¾„ï¼ˆæŒ‰é€‰æ‹©é›†ä¸å¤©æ•°åˆ†æ¡¶ï¼‰ã€‚"""
        if self.selected_codes:
            import hashlib
            join_key = ",".join(sorted(self.selected_codes))
            digest = hashlib.md5(join_key.encode("utf-8")).hexdigest()[:8]
            return os.path.join(self.cache_dir, f"daily_data_db_{days}_{digest}.pkl")
        return os.path.join(self.cache_dir, f"daily_data_db_{days}.pkl")
    
    def _get_cache_info_file(self):
        """è¿”å›å‘¨é¢‘ç¼“å­˜è¯´æ˜æ–‡ä»¶è·¯å¾„ã€‚"""
        if self.selected_codes:
            import hashlib
            join_key = ",".join(sorted(self.selected_codes))
            digest = hashlib.md5(join_key.encode("utf-8")).hexdigest()[:8]
            return os.path.join(self.cache_dir, f"cache_info_db_{digest}.txt")
        return os.path.join(self.cache_dir, "cache_info_db.txt")

    def _get_daily_cache_info_file(self, days: int = 90):
        """è¿”å›æ—¥é¢‘ç¼“å­˜è¯´æ˜æ–‡ä»¶è·¯å¾„ã€‚"""
        if self.selected_codes:
            import hashlib
            join_key = ",".join(sorted(self.selected_codes))
            digest = hashlib.md5(join_key.encode("utf-8")).hexdigest()[:8]
            return os.path.join(self.cache_dir, f"cache_info_daily_db_{days}_{digest}.txt")
        return os.path.join(self.cache_dir, f"cache_info_daily_db_{days}.txt")
    
    def _is_cache_valid(self, max_age_hours=24):
        """æ£€æŸ¥å‘¨é¢‘ç¼“å­˜æ˜¯å¦ä»ç„¶æœ‰æ•ˆï¼ˆåŸºäºæ–‡ä»¶ mtimeï¼Œé»˜è®¤ 24 å°æ—¶ï¼‰ã€‚"""
        cache_file = self._get_cache_file()
        info_file = self._get_cache_info_file()
        if not os.path.exists(cache_file) or not os.path.exists(info_file):
            return False
        cache_mtime = os.path.getmtime(cache_file)
        current_time = datetime.now().timestamp()
        return (current_time - cache_mtime) < max_age_hours * 3600

    def _is_daily_cache_valid(self, days: int = 90, max_age_hours: int = 12):
        """æ£€æŸ¥æ—¥é¢‘ç¼“å­˜æ˜¯å¦ä»ç„¶æœ‰æ•ˆï¼ˆé»˜è®¤ 12 å°æ—¶ï¼‰ã€‚"""
        cache_file = self._get_daily_cache_file(days)
        info_file = self._get_daily_cache_info_file(days)
        if not os.path.exists(cache_file) or not os.path.exists(info_file):
            return False
        cache_mtime = os.path.getmtime(cache_file)
        current_time = datetime.now().timestamp()
        return (current_time - cache_mtime) < max_age_hours * 3600
    
    def _save_cache(self):
        """æŒä¹…åŒ–å‘¨é¢‘å†…å­˜æ•°æ®åˆ°ç¼“å­˜æ–‡ä»¶ï¼Œå¹¶å†™å‡ºè¯´æ˜ä¿¡æ¯ã€‚"""
        cache_file = self._get_cache_file()
        info_file = self._get_cache_info_file()
        with open(cache_file, 'wb') as f:
            pickle.dump(self.weekly_data, f)
        with open(info_file, 'w', encoding='utf-8') as f:
            f.write("ç”Ÿæˆæ—¶é—´: {}\n".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
            f.write("æ•°æ®æº: æ•°æ®åº“\n")
            f.write("è‚¡ç¥¨æ•°é‡: {}\n".format(len(self.weekly_data)))
            f.write("æ•°æ®ç‚¹æ•°: {}\n".format(sum(len(data) for data in self.weekly_data.values())))
            if self.selected_codes:
                f.write("é€‰æ‹©é›†: {} åª (ç‹¬ç«‹ç¼“å­˜)\n".format(len(self.selected_codes)))
        print("ç¼“å­˜å·²ä¿å­˜åˆ°: {}".format(cache_file))

    def _save_daily_cache(self, days: int = 90):
        """æŒä¹…åŒ–æ—¥é¢‘å†…å­˜æ•°æ®åˆ°ç¼“å­˜æ–‡ä»¶ï¼Œå¹¶å†™å‡ºè¯´æ˜ä¿¡æ¯ã€‚"""
        cache_file = self._get_daily_cache_file(days)
        info_file = self._get_daily_cache_info_file(days)
        with open(cache_file, 'wb') as f:
            pickle.dump(self.daily_data, f)
        with open(info_file, 'w', encoding='utf-8') as f:
            f.write("ç”Ÿæˆæ—¶é—´: {}\n".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
            f.write("æ•°æ®æº: æ•°æ®åº“ï¼ˆæ—¥çº¿ï¼‰\n")
            f.write("è‚¡ç¥¨æ•°é‡: {}\n".format(len(self.daily_data)))
            f.write("å¤©æ•°: {}\n".format(days))
            if self.selected_codes:
                f.write("é€‰æ‹©é›†: {} åª (ç‹¬ç«‹ç¼“å­˜)\n".format(len(self.selected_codes)))
        print("æ—¥çº¿ç¼“å­˜å·²ä¿å­˜åˆ°: {}".format(cache_file))
    
    def _load_cache(self):
        """åŠ è½½å‘¨é¢‘ç¼“å­˜åˆ°å†…å­˜ã€‚"""
        cache_file = self._get_cache_file()
        try:
            with open(cache_file, 'rb') as f:
                self.weekly_data = pickle.load(f)
            print("ä»ç¼“å­˜åŠ è½½æ•°æ®: {} åªè‚¡ç¥¨".format(len(self.weekly_data)))
            return True
        except Exception as e:
            print("åŠ è½½ç¼“å­˜å¤±è´¥: {}".format(e))
            return False

    def _load_daily_cache(self, days: int = 90):
        """åŠ è½½æ—¥é¢‘ç¼“å­˜åˆ°å†…å­˜ã€‚"""
        cache_file = self._get_daily_cache_file(days)
        try:
            with open(cache_file, 'rb') as f:
                self.daily_data = pickle.load(f)
            print("ä»æ—¥çº¿ç¼“å­˜åŠ è½½æ•°æ®: {} åªè‚¡ç¥¨".format(len(self.daily_data)))
            return True
        except Exception as e:
            print("åŠ è½½æ—¥çº¿ç¼“å­˜å¤±è´¥: {}".format(e))
            return False
    
    def _create_connection(self):
        """åˆ›å»ºæ•°æ®åº“è¿æ¥å¼•æ“å¹¶åšä¸€æ¬¡è½»é‡æ¢æ´»ï¼ŒæˆåŠŸåå°†å¼•æ“ä¿å­˜åœ¨å®ä¾‹ä¸Šã€‚"""
        try:
            connection_string = (
                f"mysql+pymysql://{self.db_config['username']}:{self.db_config['password']}"
                f"@{self.db_config['host']}:{self.db_config['port']}"
                f"/{self.db_config['database']}?charset={self.db_config['charset']}"
            )
            
            self.engine = create_engine(
                connection_string,
                pool_pre_ping=True,
                pool_recycle=3600,
                echo=False,
                # æ€§èƒ½ä¼˜åŒ–å‚æ•°
                pool_size=10,
                max_overflow=20,
                connect_args={
                    "connect_timeout": 60,
                    "read_timeout": 300,
                    "write_timeout": 300,
                    "charset": "utf8mb4"
                }
            )
            # æ¢æ´»
            with self.engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            print("æ•°æ®åº“è¿æ¥æˆåŠŸ")
            return True
        except Exception as e:
            print(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            return False
    
    def _get_stock_codes(self):
        """è·å–è‚¡ç¥¨ä»£ç åˆ—è¡¨ã€‚è‹¥æ„é€ æ—¶æä¾›äº†é€‰æ‹©é›†ï¼Œåˆ™ç›´æ¥è¿”å›è¯¥é›†åˆã€‚"""
        if self.selected_codes:
            print(f"ä½¿ç”¨é€‰æ‹©é›†è‚¡ç¥¨ä»£ç  {len(self.selected_codes)} ä¸ª")
            return list(self.selected_codes)
        try:
            query = "SELECT stock_code FROM stock_name ORDER BY stock_code"
            df = pd.read_sql(query, self.engine)
            stock_codes = df['stock_code'].tolist()
            print(f"è·å–åˆ° {len(stock_codes)} ä¸ªè‚¡ç¥¨ä»£ç ")
            return stock_codes
        except Exception as e:
            print(f"è·å–è‚¡ç¥¨ä»£ç å¤±è´¥: {e}")
            return []
    
    def _get_weekly_data_for_stock(self, stock_code):
        """è¿”å›å•åªè‚¡ç¥¨çš„å‘¨ K çº¿ DataFrameã€‚ç´¢å¼•ä¸ºæ—¥æœŸï¼Œåˆ—ä¸º open/high/low/closeã€‚"""
        try:
            three_years_ago = (datetime.now() - timedelta(days=3*365)).strftime('%Y-%m-%d')
            query = """
            SELECT 
                trade_date,
                CAST(open AS DECIMAL(10,2)) as open,
                CAST(high AS DECIMAL(10,2)) as high,
                CAST(low  AS DECIMAL(10,2)) as low,
                CAST(close AS DECIMAL(10,2)) as close
            FROM history_week_data 
            WHERE code = %s AND trade_date >= %s
            ORDER BY trade_date
            """
            df = pd.read_sql(query, self.engine, params=(stock_code, three_years_ago))
            if len(df) == 0:
                return None
            df['trade_date'] = pd.to_datetime(df['trade_date'])
            df = df.set_index('trade_date')
            numeric_cols = ['open', 'high', 'low', 'close']
            df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')
            df = df.dropna()
            return df if len(df) > 0 else None
        except Exception:
            return None

    def _get_daily_data_for_stock(self, stock_code: str, days: int = 90):
        """è¿”å›å•åªè‚¡ç¥¨æœ€è¿‘ N å¤©ï¼ˆæ—¥å†å¤©ï¼‰å†…çš„æ—¥ K çº¿ DataFrameã€‚"""
        try:
            start_date = (datetime.now() - timedelta(days=days + 30)).strftime('%Y-%m-%d')
            query = """
            SELECT 
                trade_date,
                CAST(open AS DECIMAL(10,2)) as open,
                CAST(high AS DECIMAL(10,2)) as high,
                CAST(low  AS DECIMAL(10,2)) as low,
                CAST(close AS DECIMAL(10,2)) as close
            FROM history_day_data 
            WHERE code = %s AND trade_date >= %s
            ORDER BY trade_date
            """
            df = pd.read_sql(query, self.engine, params=(stock_code, start_date))
            if len(df) == 0:
                return None
            df['trade_date'] = pd.to_datetime(df['trade_date'])
            df = df.set_index('trade_date')
            numeric_cols = ['open', 'high', 'low', 'close']
            df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')
            df = df.dropna()
            # åªä¿ç•™æœ€è¿‘ days ä¸ªè‡ªç„¶æ—¥å†…çš„äº¤æ˜“è®°å½•
            cutoff = pd.Timestamp(datetime.now() - timedelta(days=days))
            df = df[df.index >= cutoff]
            return df if len(df) > 0 else None
        except Exception:
            return None
    
    def load_data(self):
        """å…¼å®¹æ¥å£ï¼šå»ºç«‹æ•°æ®åº“è¿æ¥ã€‚æˆåŠŸè¿”å› Trueã€‚"""
        return self._create_connection()
    
    def process_weekly_data(self):
        """åŠ è½½æ‰€æœ‰ï¼ˆæˆ–é€‰æ‹©é›†ï¼‰è‚¡ç¥¨çš„å‘¨ K çº¿æ•°æ®åˆ°å†…å­˜å¹¶ç¼“å­˜ã€‚"""
        # ä¼˜å…ˆå°è¯•ç¼“å­˜
        if self._is_cache_valid():
            if self._load_cache():
                return True
        # åŠ è½½
        if not self._create_connection():
            print("æ— æ³•è¿æ¥åˆ°æ•°æ®åº“")
            return False
        print("å¼€å§‹ä»æ•°æ®åº“åŠ è½½å‘¨Kçº¿æ•°æ®...")
        try:
            stock_codes = self._get_stock_codes()
            if not stock_codes:
                print("æœªèƒ½è·å–è‚¡ç¥¨ä»£ç åˆ—è¡¨")
                return False
            print(f"è·å–åˆ° {len(stock_codes)} ä¸ªè‚¡ç¥¨ä»£ç ")
            self._load_all_stock_data(stock_codes)
            print(f"æˆåŠŸå¤„ç† {len(self.weekly_data)} åªè‚¡ç¥¨çš„å‘¨Kçº¿æ•°æ®")
            if self.weekly_data:
                self._save_cache()
            return True
        except Exception as e:
            print(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return False

    def process_daily_data_recent(self, days: int = 90):
        """åŠ è½½æ‰€æœ‰ï¼ˆæˆ–é€‰æ‹©é›†ï¼‰è‚¡ç¥¨æœ€è¿‘ N å¤©çš„æ—¥ K çº¿æ•°æ®åˆ°å†…å­˜å¹¶ç¼“å­˜ã€‚"""
        if self._is_daily_cache_valid(days=days):
            if self._load_daily_cache(days=days):
                return True
        if not self._create_connection():
            print("æ— æ³•è¿æ¥åˆ°æ•°æ®åº“")
            return False
        print(f"å¼€å§‹ä»æ•°æ®åº“åŠ è½½æœ€è¿‘{days}å¤©çš„æ—¥Kçº¿æ•°æ®...")
        try:
            stock_codes = self._get_stock_codes()
            if not stock_codes:
                print("æœªèƒ½è·å–è‚¡ç¥¨ä»£ç åˆ—è¡¨")
                return False
            print(f"è·å–åˆ° {len(stock_codes)} ä¸ªè‚¡ç¥¨ä»£ç ")
            successful_count = 0
            total_count = len(stock_codes)
            for i, stock_code in enumerate(stock_codes, 1):
                if i % 100 == 0:
                    print(f"å¤„ç†è¿›åº¦(æ—¥): {i}/{total_count} ({i/total_count*100:.1f}%)")
                daily_df = self._get_daily_data_for_stock(stock_code, days=days)
                if daily_df is not None and len(daily_df) > 0:
                    self.daily_data[stock_code] = daily_df
                    successful_count += 1
            print(f"éå†å®Œæˆï¼ˆæ—¥çº¿ï¼‰ï¼ŒæˆåŠŸå¤„ç† {successful_count} åªè‚¡ç¥¨")
            if self.daily_data:
                self._save_daily_cache(days=days)
            return True
        except Exception as e:
            print(f"åŠ è½½æ—¥çº¿æ•°æ®å¤±è´¥: {e}")
            return False
    
    def _load_all_stock_data(self, stock_codes):
        """æŒ‰ç»™å®šåˆ—è¡¨éå†åŠ è½½å‘¨ K çº¿æ•°æ®ï¼Œå¡«å……åˆ° `self.weekly_data`ã€‚"""
        successful_count = 0
        total_count = len(stock_codes)
        print(f"å¼€å§‹éå† {total_count} åªè‚¡ç¥¨...")
        for i, stock_code in enumerate(stock_codes, 1):
            if i % 100 == 0:
                print(f"å¤„ç†è¿›åº¦: {i}/{total_count} ({i/total_count*100:.1f}%)")
            weekly_data = self._get_weekly_data_for_stock(stock_code)
            if weekly_data is not None and len(weekly_data) > 0:
                self.weekly_data[stock_code] = weekly_data
                successful_count += 1
        print(f"éå†å®Œæˆï¼ŒæˆåŠŸå¤„ç† {successful_count} åªè‚¡ç¥¨")
    
    def get_stock_codes(self):
        """è¿”å›å·²åŠ è½½çš„è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼ˆæ¥è‡ªå†…å­˜å­—å…¸é”®ï¼‰ã€‚"""
        return list(self.weekly_data.keys())
    
    def get_stock_data(self, code):
        """è¿”å›å•åªè‚¡ç¥¨çš„å‘¨ K çº¿ DataFrameï¼ˆè‹¥ä¸å­˜åœ¨åˆ™è¿”å› Noneï¼‰ã€‚"""
        return self.weekly_data.get(code)
    
    def get_all_data(self):
        """è¿”å›å…¨éƒ¨å‘¨é¢‘æ•°æ®å­—å…¸ã€‚"""
        return self.weekly_data

    def get_all_daily_data(self):
        """è¿”å›å…¨éƒ¨æ—¥é¢‘æ•°æ®å­—å…¸ã€‚"""
        return self.daily_data
    
    def close_connection(self):
        """å…³é—­æ•°æ®åº“è¿æ¥å¼•æ“ï¼ˆå¦‚å­˜åœ¨ï¼‰ã€‚"""
        if self.engine:
            self.engine.dispose()
            print("æ•°æ®åº“è¿æ¥å·²å…³é—­")


if __name__ == "__main__":
    """ç®€å•çš„æ€§èƒ½æµ‹è¯•å…¥å£ï¼šè¿æ¥â†’åŠ è½½â†’ç»Ÿè®¡ç”¨æ—¶"""
    import time
    start_time = time.time()
    processor = DatabaseStockDataProcessor()
    print("å¼€å§‹æ€§èƒ½æµ‹è¯•...")
    if processor.load_data():
        print("æ•°æ®åº“è¿æ¥æˆåŠŸ")
        if processor.process_weekly_data():
            stock_codes = processor.get_stock_codes()
            print(f"âœ… æˆåŠŸåŠ è½½ {len(stock_codes)} åªè‚¡ç¥¨çš„æ•°æ®")
            total_time = time.time() - start_time
            print(f"ğŸš€ æ€»è€—æ—¶: {total_time:.1f} ç§’")
            if len(stock_codes) > 0:
                print(f"ğŸ“Š å¹³å‡æ¯åªè‚¡ç¥¨å¤„ç†æ—¶é—´: {total_time/len(stock_codes)*1000:.2f} æ¯«ç§’")
        else:
            print("âŒ æ•°æ®å¤„ç†å¤±è´¥")
    else:
        print("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥")
    processor.close_connection()